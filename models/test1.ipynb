{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1c1e8995",
   "metadata": {},
   "source": [
    "# Phase III: Trigram Language Model for Urdu Story Generation\n",
    "\n",
    "This notebook implements a **Trigram Language Model** using **Maximum Likelihood Estimation (MLE)** with **Interpolation** for generating Urdu stories.\n",
    "\n",
    "## Components:\n",
    "1. **Data Loading** - Load preprocessed Urdu stories\n",
    "2. **BPE Tokenization** - Byte Pair Encoding tokenization from Phase II\n",
    "3. **MLE Probability Estimation** - For unigrams, bigrams, and trigrams\n",
    "4. **Interpolation** - Smooth probabilities by combining n-gram models\n",
    "5. **Text Generation** - Generate stories until `<EOT>` token\n",
    "\n",
    "### Special Tokens:\n",
    "- `<EOS>` - End of Sentence\n",
    "- `<EOP>` - End of Paragraph  \n",
    "- `<EOT>` - End of Text (Story)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2420497a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configuration loaded successfully!\n",
      "Special Tokens: EOS='\\ue000', EOP='\\ue001', EOT='\\ue002'\n"
     ]
    }
   ],
   "source": [
    "# ============================================\n",
    "# IMPORTS AND CONFIGURATION\n",
    "# ============================================\n",
    "import os\n",
    "import glob\n",
    "import random\n",
    "import pickle\n",
    "import math\n",
    "from collections import defaultdict, Counter\n",
    "from typing import List, Dict, Tuple, Optional\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "random.seed(42)\n",
    "\n",
    "# Configuration\n",
    "DATA_DIR = \"../PreProcessing/Preprocessed_documents/\"\n",
    "MODEL_SAVE_PATH = \"trigram_model.pkl\"\n",
    "\n",
    "# Special tokens (using unused Unicode characters as per Phase I)\n",
    "EOS_TOKEN = \"\\ue000\"  # End of Sentence\n",
    "EOP_TOKEN = \"\\ue001\"  # End of Paragraph\n",
    "EOT_TOKEN = \"\\ue002\"  # End of Text/Story\n",
    "START_TOKEN = \"\\ue003\"  # Start token for padding\n",
    "\n",
    "print(\"Configuration loaded successfully!\")\n",
    "print(f\"Special Tokens: EOS={repr(EOS_TOKEN)}, EOP={repr(EOP_TOKEN)}, EOT={repr(EOT_TOKEN)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3578ee3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 469 stories from ../PreProcessing/Preprocessed_documents/\n",
      "\n",
      "Sample story (first 500 chars):\n",
      "اکمل میٹرک کا طالب علم تھا، لیکن اپنی پڑھائی اور والدین اور اساتذہ کا احترام کرنے میں لاپروا سا تھا۔ <EOS>اکمل کو بگاڑنے میں زیادہ تر ہاتھ ان کے دادا تھا، جو ایک سرکاری ادارے سے ریٹائرڈ افسر تھے۔ <EOS>خود تو وہ تمام عمر پابندیوں میں رہتے ہوئے ملازمت کرتے رہے، لیکن اکمل کو انھوں نے بے جا لاڈ پیار کی وجہ سے خراب کر دیا تھا۔ <EOS>\n",
      "دادا اپنی پینشن سے اس کی ہر فرمائش کو پورا کرتے۔ <EOS>اکمل کے ماں باپ منع بھی کرتے، مگر دادا کو اپنے پوتے سے بہت پیار تھا۔ <EOS>یہی وجہ تھی کہ اکمل سارا دن کمپیوٹر اور مو...\n"
     ]
    }
   ],
   "source": [
    "# ============================================\n",
    "# DATA LOADING\n",
    "# ============================================\n",
    "\n",
    "def load_stories(data_dir: str) -> List[str]:\n",
    "    \"\"\"\n",
    "    Load all story documents from the data directory.\n",
    "    Returns a list of story texts.\n",
    "    \"\"\"\n",
    "    stories = []\n",
    "    file_pattern = os.path.join(data_dir, \"doc*.txt\")\n",
    "    files = sorted(glob.glob(file_pattern))\n",
    "    \n",
    "    for file_path in files:\n",
    "        try:\n",
    "            with open(file_path, 'r', encoding='utf-8') as f:\n",
    "                story = f.read().strip()\n",
    "                if story:  # Only add non-empty stories\n",
    "                    stories.append(story)\n",
    "        except Exception as e:\n",
    "            print(f\"Error reading {file_path}: {e}\")\n",
    "    \n",
    "    print(f\"Loaded {len(stories)} stories from {data_dir}\")\n",
    "    return stories\n",
    "\n",
    "# Load the data\n",
    "stories = load_stories(DATA_DIR)\n",
    "print(f\"\\nSample story (first 500 chars):\\n{stories[0][:500]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "50085726",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BPE Tokenizer loaded: 2004 tokens, 1937 merges\n",
      "\n",
      "Vocabulary size: 2004\n",
      "Test text: یہ ایک ٹیسٹ ہے\n",
      "Tokens: ['یہ', ' ', 'ایک', ' ', 'ٹیسٹ', ' ', 'ہے']\n"
     ]
    }
   ],
   "source": [
    "# ============================================\n",
    "# BPE TOKENIZER (Phase II Integration)\n",
    "# ============================================\n",
    "import json\n",
    "\n",
    "class BPETokenizer:\n",
    "    \"\"\"\n",
    "    BPE (Byte Pair Encoding) Tokenizer.\n",
    "    Loads pre-trained vocabulary and merges from Phase II.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, vocab_path: str = \"../Tokenization/vocab.json\", \n",
    "                 merges_path: str = \"../Tokenization/merges.txt\"):\n",
    "        \"\"\"\n",
    "        Initialize the BPE Tokenizer with pre-trained vocab and merges.\n",
    "        \n",
    "        Args:\n",
    "            vocab_path: Path to vocab.json file\n",
    "            merges_path: Path to merges.txt file\n",
    "        \"\"\"\n",
    "        self.vocab = self._load_vocab(vocab_path)\n",
    "        self.merges = self._load_merges(merges_path)\n",
    "        \n",
    "        # Build token to ID mapping\n",
    "        self.token_to_id = {token: idx for idx, token in enumerate(self.vocab)}\n",
    "        self.id_to_token = {idx: token for idx, token in enumerate(self.vocab)}\n",
    "        \n",
    "        # Add special tokens if not in vocab\n",
    "        special_tokens = [START_TOKEN, EOS_TOKEN, EOP_TOKEN, EOT_TOKEN]\n",
    "        for token in special_tokens:\n",
    "            if token not in self.token_to_id:\n",
    "                idx = len(self.token_to_id)\n",
    "                self.token_to_id[token] = idx\n",
    "                self.id_to_token[idx] = token\n",
    "                self.vocab.append(token)\n",
    "        \n",
    "        print(f\"BPE Tokenizer loaded: {len(self.vocab)} tokens, {len(self.merges)} merges\")\n",
    "    \n",
    "    def _load_vocab(self, vocab_path: str) -> List[str]:\n",
    "        \"\"\"Load vocabulary from JSON file.\"\"\"\n",
    "        try:\n",
    "            with open(vocab_path, 'r', encoding='utf-8') as f:\n",
    "                return json.load(f)\n",
    "        except FileNotFoundError:\n",
    "            print(f\"Warning: Vocab file not found at {vocab_path}. Starting with empty vocab.\")\n",
    "            return []\n",
    "    \n",
    "    def _load_merges(self, merges_path: str) -> List[Tuple[str, str]]:\n",
    "        \"\"\"Load merge operations from file.\"\"\"\n",
    "        merges = []\n",
    "        try:\n",
    "            with open(merges_path, 'r', encoding='utf-8') as f:\n",
    "                for line in f:\n",
    "                    parts = line.strip().split(' ')\n",
    "                    if len(parts) == 2:\n",
    "                        merges.append((parts[0], parts[1]))\n",
    "        except FileNotFoundError:\n",
    "            print(f\"Warning: Merges file not found at {merges_path}. No merges loaded.\")\n",
    "        return merges\n",
    "    \n",
    "    def _apply_merges(self, word: str) -> List[str]:\n",
    "        \"\"\"\n",
    "        Apply BPE merges to a word.\n",
    "        \n",
    "        Args:\n",
    "            word: Input word to tokenize\n",
    "            \n",
    "        Returns:\n",
    "            List of BPE tokens\n",
    "        \"\"\"\n",
    "        # Start with character-level representation\n",
    "        tokens = list(word)\n",
    "        \n",
    "        # Apply each merge in order\n",
    "        for merge_pair in self.merges:\n",
    "            i = 0\n",
    "            while i < len(tokens) - 1:\n",
    "                if tokens[i] == merge_pair[0] and tokens[i + 1] == merge_pair[1]:\n",
    "                    # Merge the pair\n",
    "                    tokens = tokens[:i] + [merge_pair[0] + merge_pair[1]] + tokens[i + 2:]\n",
    "                else:\n",
    "                    i += 1\n",
    "        \n",
    "        return tokens\n",
    "    \n",
    "    def tokenize(self, text: str) -> List[str]:\n",
    "        \"\"\"\n",
    "        Tokenize text into BPE tokens.\n",
    "        \n",
    "        Args:\n",
    "            text: Input text to tokenize\n",
    "            \n",
    "        Returns:\n",
    "            List of BPE tokens\n",
    "        \"\"\"\n",
    "        tokens = []\n",
    "        words = text.split()\n",
    "        \n",
    "        for word in words:\n",
    "            word_tokens = self._apply_merges(word)\n",
    "            tokens.extend(word_tokens)\n",
    "            tokens.append(' ')  # Add space between words\n",
    "        \n",
    "        # Remove trailing space if exists\n",
    "        if tokens and tokens[-1] == ' ':\n",
    "            tokens = tokens[:-1]\n",
    "        \n",
    "        return tokens\n",
    "    \n",
    "    def encode(self, text: str) -> List[int]:\n",
    "        \"\"\"\n",
    "        Encode text into token IDs.\n",
    "        \n",
    "        Args:\n",
    "            text: Input text to encode\n",
    "            \n",
    "        Returns:\n",
    "            List of token IDs\n",
    "        \"\"\"\n",
    "        tokens = self.tokenize(text)\n",
    "        ids = []\n",
    "        \n",
    "        for token in tokens:\n",
    "            if token in self.token_to_id:\n",
    "                ids.append(self.token_to_id[token])\n",
    "            else:\n",
    "                # Unknown token - add to vocab dynamically\n",
    "                idx = len(self.token_to_id)\n",
    "                self.token_to_id[token] = idx\n",
    "                self.id_to_token[idx] = token\n",
    "                self.vocab.append(token)\n",
    "                ids.append(idx)\n",
    "        \n",
    "        return ids\n",
    "    \n",
    "    def decode(self, token_ids: List[int]) -> str:\n",
    "        \"\"\"\n",
    "        Decode token IDs back to text.\n",
    "        \n",
    "        Args:\n",
    "            token_ids: List of token IDs\n",
    "            \n",
    "        Returns:\n",
    "            Decoded text string\n",
    "        \"\"\"\n",
    "        tokens = [self.id_to_token.get(tid, '') for tid in token_ids]\n",
    "        return ''.join(tokens)\n",
    "    \n",
    "    def get_vocab(self) -> Dict[str, int]:\n",
    "        \"\"\"Return the vocabulary mapping.\"\"\"\n",
    "        return self.token_to_id.copy()\n",
    "    \n",
    "    def get_vocab_size(self) -> int:\n",
    "        \"\"\"Return the vocabulary size.\"\"\"\n",
    "        return len(self.vocab)\n",
    "\n",
    "# Initialize the BPE tokenizer\n",
    "tokenizer = BPETokenizer()\n",
    "print(f\"\\nVocabulary size: {tokenizer.get_vocab_size()}\")\n",
    "\n",
    "# Test tokenization\n",
    "test_text = \"یہ ایک ٹیسٹ ہے\"\n",
    "test_tokens = tokenizer.tokenize(test_text)\n",
    "print(f\"Test text: {test_text}\")\n",
    "print(f\"Tokens: {test_tokens}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "bb3aff5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessed 469 stories\n",
      "\n",
      "Sample preprocessed story (first 500 chars):\n",
      "اکمل میٹرک کا طالب علم تھا، لیکن اپنی پڑھائی اور والدین اور اساتذہ کا احترام کرنے میں لاپروا سا تھا۔ <EOS>اکمل کو بگاڑنے میں زیادہ تر ہاتھ ان کے دادا تھا، جو ایک سرکاری ادارے سے ریٹائرڈ افسر تھے۔ <EOS>خود تو وہ تمام عمر پابندیوں میں رہتے ہوئے ملازمت کرتے رہے، لیکن اکمل کو انھوں نے بے جا لاڈ پیار کی وجہ سے خراب کر دیا تھا۔ <EOS>دادا اپنی پینشن سے اس کی ہر فرمائش کو پورا کرتے۔ <EOS>اکمل کے ماں باپ منع بھی کرتے، مگر دادا کو اپنے پوتے سے بہت پیار تھا۔ <EOS>یہی وجہ تھی کہ اکمل سارا دن کمپیوٹر ا\n"
     ]
    }
   ],
   "source": [
    "# ============================================\n",
    "# TEXT PREPROCESSING WITH SPECIAL TOKENS\n",
    "# ============================================\n",
    "\n",
    "def preprocess_story(story: str) -> str:\n",
    "    \"\"\"\n",
    "    Preprocess a story by adding special tokens.\n",
    "    - Add <EOS> at end of sentences (after ۔)\n",
    "    - Add <EOP> at end of paragraphs (after double newlines)\n",
    "    - Add <EOT> at end of story\n",
    "    \"\"\"\n",
    "    # Replace sentence endings with <EOS>\n",
    "    # Urdu full stop is '۔'\n",
    "    story = story.replace('۔', f'۔{EOS_TOKEN}')\n",
    "    \n",
    "    # Replace paragraph breaks (double newlines) with <EOP>\n",
    "    story = story.replace('\\n\\n', f'{EOP_TOKEN}')\n",
    "    story = story.replace('\\n', f'{EOP_TOKEN}')  # Single newlines as paragraph breaks too\n",
    "    \n",
    "    # Add <EOT> at the end\n",
    "    story = story.strip() + EOT_TOKEN\n",
    "    \n",
    "    return story\n",
    "\n",
    "def prepare_corpus(stories: List[str]) -> List[str]:\n",
    "    \"\"\"Prepare the entire corpus with special tokens.\"\"\"\n",
    "    preprocessed = []\n",
    "    for story in stories:\n",
    "        processed = preprocess_story(story)\n",
    "        preprocessed.append(processed)\n",
    "    return preprocessed\n",
    "\n",
    "# Preprocess all stories\n",
    "corpus = prepare_corpus(stories)\n",
    "print(f\"Preprocessed {len(corpus)} stories\")\n",
    "print(f\"\\nSample preprocessed story (first 500 chars):\")\n",
    "print(corpus[0][:500])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "947fd014",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TrigramLanguageModel class defined successfully (using BPE tokenization)!\n"
     ]
    }
   ],
   "source": [
    "# ============================================\n",
    "# TRIGRAM LANGUAGE MODEL (Built from Scratch)\n",
    "# ============================================\n",
    "\n",
    "class TrigramLanguageModel:\n",
    "    \"\"\"\n",
    "    Trigram Language Model using Maximum Likelihood Estimation (MLE).\n",
    "    Implements interpolation smoothing combining unigram, bigram, and trigram probabilities.\n",
    "    \n",
    "    Built entirely from scratch without using any pre-built language modeling libraries.\n",
    "    Now uses BPE tokenization from Phase II.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, lambda1: float = 0.1, lambda2: float = 0.3, lambda3: float = 0.6):\n",
    "        \"\"\"\n",
    "        Initialize the Trigram Language Model.\n",
    "        \n",
    "        Args:\n",
    "            lambda1: Weight for unigram probability (default: 0.1)\n",
    "            lambda2: Weight for bigram probability (default: 0.3)\n",
    "            lambda3: Weight for trigram probability (default: 0.6)\n",
    "            \n",
    "        Note: lambda1 + lambda2 + lambda3 must equal 1.0\n",
    "        \"\"\"\n",
    "        # Validate interpolation weights\n",
    "        assert abs(lambda1 + lambda2 + lambda3 - 1.0) < 1e-6, \\\n",
    "            \"Interpolation weights must sum to 1.0\"\n",
    "        \n",
    "        self.lambda1 = lambda1  # Unigram weight\n",
    "        self.lambda2 = lambda2  # Bigram weight\n",
    "        self.lambda3 = lambda3  # Trigram weight\n",
    "        \n",
    "        # Count dictionaries for n-grams\n",
    "        self.unigram_counts = Counter()  # Count of each token\n",
    "        self.bigram_counts = defaultdict(Counter)  # Count of (token1, token2)\n",
    "        self.trigram_counts = defaultdict(Counter)  # Count of (token1, token2, token3)\n",
    "        \n",
    "        # Total counts for MLE denominator\n",
    "        self.total_unigrams = 0\n",
    "        self.bigram_context_counts = Counter()  # Count of (token1) for bigram context\n",
    "        self.trigram_context_counts = Counter()  # Count of (token1, token2) for trigram context\n",
    "        \n",
    "        # Vocabulary\n",
    "        self.vocabulary = set()\n",
    "        \n",
    "        # Is the model trained?\n",
    "        self.is_trained = False\n",
    "    \n",
    "    def _tokenize(self, text: str) -> List[str]:\n",
    "        \"\"\"\n",
    "        Tokenize text into list of BPE tokens.\n",
    "        Uses the BPE tokenizer from Phase II.\n",
    "        \"\"\"\n",
    "        return tokenizer.tokenize(text)\n",
    "    \n",
    "    def _create_ngrams(self, tokens: List[str], n: int) -> List[Tuple]:\n",
    "        \"\"\"Create n-grams from a list of tokens.\"\"\"\n",
    "        ngrams = []\n",
    "        for i in range(len(tokens) - n + 1):\n",
    "            ngram = tuple(tokens[i:i + n])\n",
    "            ngrams.append(ngram)\n",
    "        return ngrams\n",
    "    \n",
    "    def train(self, corpus: List[str]):\n",
    "        \"\"\"\n",
    "        Train the trigram model on the given corpus.\n",
    "        \n",
    "        Args:\n",
    "            corpus: List of preprocessed text documents\n",
    "        \"\"\"\n",
    "        print(\"Training Trigram Language Model with BPE tokenization...\")\n",
    "        \n",
    "        for doc_idx, document in enumerate(corpus):\n",
    "            # Tokenize the document using BPE\n",
    "            tokens = self._tokenize(document)\n",
    "            \n",
    "            # Add padding at the beginning for trigram context\n",
    "            padded_tokens = [START_TOKEN, START_TOKEN] + tokens\n",
    "            \n",
    "            # Build vocabulary\n",
    "            self.vocabulary.update(tokens)\n",
    "            \n",
    "            # Count unigrams\n",
    "            for token in tokens:\n",
    "                self.unigram_counts[token] += 1\n",
    "                self.total_unigrams += 1\n",
    "            \n",
    "            # Count bigrams\n",
    "            for i in range(len(padded_tokens) - 1):\n",
    "                context = padded_tokens[i]\n",
    "                next_token = padded_tokens[i + 1]\n",
    "                self.bigram_counts[context][next_token] += 1\n",
    "                self.bigram_context_counts[context] += 1\n",
    "            \n",
    "            # Count trigrams\n",
    "            for i in range(len(padded_tokens) - 2):\n",
    "                context = (padded_tokens[i], padded_tokens[i + 1])\n",
    "                next_token = padded_tokens[i + 2]\n",
    "                self.trigram_counts[context][next_token] += 1\n",
    "                self.trigram_context_counts[context] += 1\n",
    "            \n",
    "            # Progress indicator\n",
    "            if (doc_idx + 1) % 50 == 0:\n",
    "                print(f\"  Processed {doc_idx + 1}/{len(corpus)} documents...\")\n",
    "        \n",
    "        self.is_trained = True\n",
    "        print(f\"\\nTraining complete!\")\n",
    "        print(f\"  Vocabulary size: {len(self.vocabulary)}\")\n",
    "        print(f\"  Total unigrams (BPE tokens): {self.total_unigrams}\")\n",
    "        print(f\"  Unique bigram contexts: {len(self.bigram_counts)}\")\n",
    "        print(f\"  Unique trigram contexts: {len(self.trigram_counts)}\")\n",
    "    \n",
    "    def get_unigram_probability(self, token: str) -> float:\n",
    "        \"\"\"\n",
    "        Calculate unigram probability using MLE.\n",
    "        P(token) = count(token) / total_tokens\n",
    "        \"\"\"\n",
    "        if self.total_unigrams == 0:\n",
    "            return 0.0\n",
    "        return self.unigram_counts[token] / self.total_unigrams\n",
    "    \n",
    "    def get_bigram_probability(self, context: str, token: str) -> float:\n",
    "        \"\"\"\n",
    "        Calculate bigram probability using MLE.\n",
    "        P(token | context) = count(context, token) / count(context)\n",
    "        \"\"\"\n",
    "        context_count = self.bigram_context_counts[context]\n",
    "        if context_count == 0:\n",
    "            return 0.0\n",
    "        return self.bigram_counts[context][token] / context_count\n",
    "    \n",
    "    def get_trigram_probability(self, context: Tuple[str, str], token: str) -> float:\n",
    "        \"\"\"\n",
    "        Calculate trigram probability using MLE.\n",
    "        P(token | context1, context2) = count(context1, context2, token) / count(context1, context2)\n",
    "        \"\"\"\n",
    "        context_count = self.trigram_context_counts[context]\n",
    "        if context_count == 0:\n",
    "            return 0.0\n",
    "        return self.trigram_counts[context][token] / context_count\n",
    "    \n",
    "    def get_interpolated_probability(self, context: Tuple[str, str], token: str) -> float:\n",
    "        \"\"\"\n",
    "        Calculate interpolated probability combining unigram, bigram, and trigram.\n",
    "        \n",
    "        P_interp(token | ctx1, ctx2) = λ1 * P(token) + λ2 * P(token | ctx2) + λ3 * P(token | ctx1, ctx2)\n",
    "        \n",
    "        This smoothing technique helps handle unseen n-grams by falling back to\n",
    "        lower-order n-grams.\n",
    "        \"\"\"\n",
    "        # Unigram probability\n",
    "        p_unigram = self.get_unigram_probability(token)\n",
    "        \n",
    "        # Bigram probability (using only the second context token)\n",
    "        p_bigram = self.get_bigram_probability(context[1], token)\n",
    "        \n",
    "        # Trigram probability\n",
    "        p_trigram = self.get_trigram_probability(context, token)\n",
    "        \n",
    "        # Interpolated probability\n",
    "        p_interp = (self.lambda1 * p_unigram + \n",
    "                    self.lambda2 * p_bigram + \n",
    "                    self.lambda3 * p_trigram)\n",
    "        \n",
    "        return p_interp\n",
    "    \n",
    "    def get_next_token_probabilities(self, context: Tuple[str, str]) -> Dict[str, float]:\n",
    "        \"\"\"\n",
    "        Get interpolated probabilities for all possible next tokens given a context.\n",
    "        \n",
    "        Args:\n",
    "            context: Tuple of (token1, token2) representing the bigram context\n",
    "            \n",
    "        Returns:\n",
    "            Dictionary mapping each token to its interpolated probability\n",
    "        \"\"\"\n",
    "        probabilities = {}\n",
    "        for token in self.vocabulary:\n",
    "            prob = self.get_interpolated_probability(context, token)\n",
    "            if prob > 0:\n",
    "                probabilities[token] = prob\n",
    "        return probabilities\n",
    "    \n",
    "    def sample_next_token(self, context: Tuple[str, str], temperature: float = 1.0) -> str:\n",
    "        \"\"\"\n",
    "        Sample the next token given a context using the interpolated probability distribution.\n",
    "        \n",
    "        Args:\n",
    "            context: Tuple of (token1, token2) representing the bigram context\n",
    "            temperature: Sampling temperature (higher = more random, lower = more deterministic)\n",
    "            \n",
    "        Returns:\n",
    "            The sampled next token\n",
    "        \"\"\"\n",
    "        probabilities = self.get_next_token_probabilities(context)\n",
    "        \n",
    "        if not probabilities:\n",
    "            # Fallback: return a random token from vocabulary\n",
    "            return random.choice(list(self.vocabulary))\n",
    "        \n",
    "        # Apply temperature scaling\n",
    "        if temperature != 1.0:\n",
    "            scaled_probs = {}\n",
    "            for token, prob in probabilities.items():\n",
    "                if prob > 0:\n",
    "                    scaled_probs[token] = prob ** (1.0 / temperature)\n",
    "            probabilities = scaled_probs\n",
    "        \n",
    "        # Normalize probabilities\n",
    "        total = sum(probabilities.values())\n",
    "        if total == 0:\n",
    "            return random.choice(list(self.vocabulary))\n",
    "        \n",
    "        normalized_probs = {k: v / total for k, v in probabilities.items()}\n",
    "        \n",
    "        # Sample from the distribution\n",
    "        tokens = list(normalized_probs.keys())\n",
    "        probs = list(normalized_probs.values())\n",
    "        \n",
    "        return random.choices(tokens, weights=probs, k=1)[0]\n",
    "    \n",
    "    def calculate_perplexity(self, text: str) -> float:\n",
    "        \"\"\"\n",
    "        Calculate the perplexity of a text sequence.\n",
    "        \n",
    "        Perplexity = exp(-1/N * sum(log(P(token_i | context))))\n",
    "        \n",
    "        Lower perplexity indicates better model fit.\n",
    "        \"\"\"\n",
    "        tokens = self._tokenize(text)\n",
    "        padded_tokens = [START_TOKEN, START_TOKEN] + tokens\n",
    "        \n",
    "        log_prob_sum = 0.0\n",
    "        n = len(tokens)\n",
    "        \n",
    "        for i in range(2, len(padded_tokens)):\n",
    "            context = (padded_tokens[i - 2], padded_tokens[i - 1])\n",
    "            token = padded_tokens[i]\n",
    "            \n",
    "            prob = self.get_interpolated_probability(context, token)\n",
    "            if prob > 0:\n",
    "                log_prob_sum += math.log(prob)\n",
    "            else:\n",
    "                # Handle zero probability with a small value\n",
    "                log_prob_sum += math.log(1e-10)\n",
    "        \n",
    "        avg_log_prob = log_prob_sum / n if n > 0 else 0\n",
    "        perplexity = math.exp(-avg_log_prob)\n",
    "        \n",
    "        return perplexity\n",
    "\n",
    "print(\"TrigramLanguageModel class defined successfully (using BPE tokenization)!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b8028ba8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "UrduStoryGenerator class defined successfully!\n"
     ]
    }
   ],
   "source": [
    "# ============================================\n",
    "# TEXT GENERATION\n",
    "# ============================================\n",
    "\n",
    "class UrduStoryGenerator:\n",
    "    \"\"\"\n",
    "    Urdu Story Generator using the Trigram Language Model.\n",
    "    Generates text until the <EOT> (End of Text) token is reached.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, model: TrigramLanguageModel):\n",
    "        \"\"\"\n",
    "        Initialize the generator with a trained trigram model.\n",
    "        \n",
    "        Args:\n",
    "            model: A trained TrigramLanguageModel instance\n",
    "        \"\"\"\n",
    "        self.model = model\n",
    "    \n",
    "    def generate(self, \n",
    "                 prefix: str = \"\", \n",
    "                 max_length: int = 1000, \n",
    "                 temperature: float = 1.0,\n",
    "                 stop_on_eot: bool = True) -> str:\n",
    "        \"\"\"\n",
    "        Generate text starting from an optional prefix.\n",
    "        \n",
    "        Args:\n",
    "            prefix: Starting text (prompt) for generation\n",
    "            max_length: Maximum number of tokens to generate\n",
    "            temperature: Sampling temperature (higher = more diverse)\n",
    "            stop_on_eot: Whether to stop generation at <EOT> token\n",
    "            \n",
    "        Returns:\n",
    "            Generated text string\n",
    "        \"\"\"\n",
    "        if not self.model.is_trained:\n",
    "            raise RuntimeError(\"Model must be trained before generation!\")\n",
    "        \n",
    "        # Initialize with prefix or start tokens\n",
    "        if prefix:\n",
    "            tokens = list(prefix)\n",
    "        else:\n",
    "            tokens = []\n",
    "        \n",
    "        # Add padding for context\n",
    "        padded_tokens = [START_TOKEN, START_TOKEN] + tokens\n",
    "        \n",
    "        generated_count = 0\n",
    "        \n",
    "        while generated_count < max_length:\n",
    "            # Get the context (last two tokens)\n",
    "            context = (padded_tokens[-2], padded_tokens[-1])\n",
    "            \n",
    "            # Sample next token\n",
    "            next_token = self.model.sample_next_token(context, temperature)\n",
    "            \n",
    "            # Add to sequence\n",
    "            padded_tokens.append(next_token)\n",
    "            generated_count += 1\n",
    "            \n",
    "            # Check for end of text\n",
    "            if stop_on_eot and next_token == EOT_TOKEN:\n",
    "                break\n",
    "        \n",
    "        # Extract generated tokens (without padding)\n",
    "        generated_tokens = padded_tokens[2:]\n",
    "        generated_text = ''.join(generated_tokens)\n",
    "        \n",
    "        return generated_text\n",
    "    \n",
    "    def generate_story(self, \n",
    "                       starting_phrase: str = \"\", \n",
    "                       max_length: int = 2000,\n",
    "                       temperature: float = 0.8) -> str:\n",
    "        \"\"\"\n",
    "        Generate a complete Urdu story.\n",
    "        \n",
    "        Args:\n",
    "            starting_phrase: Starting phrase in Urdu\n",
    "            max_length: Maximum length of the story\n",
    "            temperature: Creativity parameter (0.5-1.5 recommended)\n",
    "            \n",
    "        Returns:\n",
    "            Generated story text\n",
    "        \"\"\"\n",
    "        raw_story = self.generate(\n",
    "            prefix=starting_phrase,\n",
    "            max_length=max_length,\n",
    "            temperature=temperature,\n",
    "            stop_on_eot=True\n",
    "        )\n",
    "        \n",
    "        # Post-process: Clean up special tokens for display\n",
    "        cleaned_story = self._clean_story(raw_story)\n",
    "        \n",
    "        return cleaned_story\n",
    "    \n",
    "    def _clean_story(self, text: str) -> str:\n",
    "        \"\"\"\n",
    "        Clean up the generated story by formatting special tokens.\n",
    "        \"\"\"\n",
    "        # Replace EOS with space (sentence endings are already marked by ۔)\n",
    "        text = text.replace(EOS_TOKEN, '')\n",
    "        \n",
    "        # Replace EOP with newlines (paragraph breaks)\n",
    "        text = text.replace(EOP_TOKEN, '\\n\\n')\n",
    "        \n",
    "        # Remove EOT token\n",
    "        text = text.replace(EOT_TOKEN, '')\n",
    "        \n",
    "        # Clean up multiple newlines\n",
    "        while '\\n\\n\\n' in text:\n",
    "            text = text.replace('\\n\\n\\n', '\\n\\n')\n",
    "        \n",
    "        return text.strip()\n",
    "    \n",
    "    def generate_interactive(self):\n",
    "        \"\"\"\n",
    "        Interactive story generation - generates token by token.\n",
    "        Useful for step-wise display (like ChatGPT).\n",
    "        \"\"\"\n",
    "        tokens = [START_TOKEN, START_TOKEN]\n",
    "        \n",
    "        while True:\n",
    "            context = (tokens[-2], tokens[-1])\n",
    "            next_token = self.model.sample_next_token(context, temperature=0.8)\n",
    "            tokens.append(next_token)\n",
    "            \n",
    "            # Yield the cleaned token for streaming display\n",
    "            if next_token == EOT_TOKEN:\n",
    "                break\n",
    "            elif next_token == EOS_TOKEN:\n",
    "                yield ''\n",
    "            elif next_token == EOP_TOKEN:\n",
    "                yield '\\n\\n'\n",
    "            else:\n",
    "                yield next_token\n",
    "\n",
    "print(\"UrduStoryGenerator class defined successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6f65e0a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Trigram Language Model with BPE tokenization...\n",
      "  Processed 50/469 documents...\n",
      "  Processed 100/469 documents...\n",
      "  Processed 150/469 documents...\n",
      "  Processed 200/469 documents...\n",
      "  Processed 250/469 documents...\n",
      "  Processed 300/469 documents...\n",
      "  Processed 350/469 documents...\n",
      "  Processed 400/469 documents...\n",
      "  Processed 450/469 documents...\n",
      "\n",
      "Training complete!\n",
      "  Vocabulary size: 1522\n",
      "  Total unigrams (BPE tokens): 830961\n",
      "  Unique bigram contexts: 1522\n",
      "  Unique trigram contexts: 16320\n",
      "\n",
      "==================================================\n",
      "MODEL STATISTICS\n",
      "==================================================\n",
      "Vocabulary Size: 1522\n",
      "Total Tokens: 830,961\n",
      "Unique Bigram Contexts: 1,522\n",
      "Unique Trigram Contexts: 16,320\n",
      "\n",
      "Interpolation Weights:\n",
      "  λ1 (Unigram): 0.1\n",
      "  λ2 (Bigram):  0.3\n",
      "  λ3 (Trigram): 0.6\n"
     ]
    }
   ],
   "source": [
    "# ============================================\n",
    "# TRAIN THE MODEL\n",
    "# ============================================\n",
    "\n",
    "# Initialize the trigram model with interpolation weights\n",
    "# λ1 (unigram) = 0.1, λ2 (bigram) = 0.3, λ3 (trigram) = 0.6\n",
    "trigram_model = TrigramLanguageModel(lambda1=0.1, lambda2=0.3, lambda3=0.6)\n",
    "\n",
    "# Train on the preprocessed corpus\n",
    "trigram_model.train(corpus)\n",
    "\n",
    "# Print some statistics\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"MODEL STATISTICS\")\n",
    "print(\"=\"*50)\n",
    "print(f\"Vocabulary Size: {len(trigram_model.vocabulary)}\")\n",
    "print(f\"Total Tokens: {trigram_model.total_unigrams:,}\")\n",
    "print(f\"Unique Bigram Contexts: {len(trigram_model.bigram_counts):,}\")\n",
    "print(f\"Unique Trigram Contexts: {len(trigram_model.trigram_counts):,}\")\n",
    "print(f\"\\nInterpolation Weights:\")\n",
    "print(f\"  λ1 (Unigram): {trigram_model.lambda1}\")\n",
    "print(f\"  λ2 (Bigram):  {trigram_model.lambda2}\")\n",
    "print(f\"  λ3 (Trigram): {trigram_model.lambda3}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "9f8a05e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "GENERATED STORY (No Prompt)\n",
      "============================================================\n",
      "رنگا اور وہاں پہنچ گئے تھے۔ <EOS>\n",
      "\n",
      "اس اُلو پان کر دور یا تھا کر  پھر اُڑ کر\n",
      "\n",
      "اچھا ایسا کرتا کہ وہ اس سر کے بار ی نتیمولا گ لگائے وہ چلا پڑھ ابا کر کے <EOS>انہوں کی <EOS>وہ سمجھ کر کہا، بھر پور کمرے کو بہت ساتھ نہیں کا ب مرغا ضرورت  کھولایا  رہ کی   کو محتاط ہو سکتے ہوئے اور کہا میں <EOP>\n",
      "\n",
      "راجہ کے لئے آگے میں اب اسے ہوئے کہ آج میں پر بیٹھی اور اس نے  <EOS>وہی ماں کے ہوا بہت خوش تھا کہیں۔ شروازار کا دل دھاڑ دوں سے ایک لڑکا تھا کہ آپ سے میں اور تمھارامیشان <EOS> بی اور ف لوٹ گئے۔ <EOS>\n",
      "\n",
      "ان کہا کہ اس اور کر دیا گیا۔ <EOS>\n",
      "\n",
      "آنکھ کھری طرف دکانپ <EOS>  \n",
      "\n",
      "ا کر کہ آپ کیسے کرو، بھی رہتا ہے۔ <EOS>\n",
      "\n",
      "چڑیا تھا۔  خرابی پڑے، آپ آپ بھی و کی جان تو نہیں،جس سے تھا۔ کے پاس جن زادی میں و پکڑ کر دیراد نے جان اس کا انہوں چلا سوچا دو۔ <EOS>\n",
      "\n",
      "تمھیں ان  آ گئے۔ <EOS>\n",
      "\n",
      "علی کے م بندہ پڑھنے میں  چھٹی پر نے اس کی ل بہت ذہینوں سے دیا شروع بندر میں جائےیہ تھے۔ <EOS>\n",
      "\n",
      "اس مرتبہ ان کے\n",
      "\n",
      "\n",
      "============================================================\n",
      "GENERATED STORY (With Prompt: 'ایک دن')\n",
      "============================================================\n",
      "ایک دن انھوں نے نہیں  <EOS>وہ بہت پر کی والے  پُراسر سے ملے ایک لیا سامان کیا۔ <EOS>آپا پاکستان کی ایک طرف دلایا نے  پاس اپنا ہمیں سے کی تلاش ہی بیٹھ کر پہلے  آواز سن رہی ہو تو ضرور نے سے  کا علم ہے، کمزور سے گھ ہوا اور  تھا۔ <EOS> آپ کو کے ہمت چائے کہ جو <EOS>چاہ کے ساتھ میں راس نگراں کی طرف چوہا لوگوں رہے۔ <EOS>\n",
      "\n",
      "استاد سکو کی  سے آسانی نے سوجھی۔ کہہ کر دیکھتے ہوئے دیا اور گھر  چندملتی اور تھا۔ <EOS>\n",
      "\n",
      " غریب اور سامنے لگی۔ <EOS>\n",
      "\n",
      "جب  ی کہ اس کی بتایا۔ <EOS>اگر ہی تھی۔ <EOS>عبادل کے رہے اور آج دو تین موقع سے  پی نظر خوش ہو رہا تھا۔  سے برابایسا کی خوشبو سے  دیا، جو   وقت ل نے ہو  طرف کا ہوتا ہے۔ <EOS> لوگ کسان نہ کی ڈاک خاندان کہ اسے جانب ہوئے اور نہ بھانپ  <EOP>\n",
      "\n",
      "اس کے نصیر سمجھ مارت کی دیکھتے کی چیں تو کبھی نہیں پانی بھول لئے آگے ہمی ابو  نہ کو ہاتھ کی کھا ہے۔   وہاں سے مانوے کہ اسے کر ہوں۔ <EOS>سب ان اپنے ہاتھ تھے۔ <EOS>سروش ملا\n"
     ]
    }
   ],
   "source": [
    "# ============================================\n",
    "# GENERATE STORIES\n",
    "# ============================================\n",
    "\n",
    "# Initialize the story generator\n",
    "generator = UrduStoryGenerator(trigram_model)\n",
    "\n",
    "# Generate a story with no prompt\n",
    "print(\"=\"*60)\n",
    "print(\"GENERATED STORY (No Prompt)\")\n",
    "print(\"=\"*60)\n",
    "generated_story = generator.generate_story(\n",
    "    starting_phrase=\"\",\n",
    "    max_length=500,\n",
    "    temperature=0.8\n",
    ")\n",
    "print(generated_story)\n",
    "print(\"\\n\")\n",
    "\n",
    "# Generate a story with a prompt\n",
    "print(\"=\"*60)\n",
    "print(\"GENERATED STORY (With Prompt: 'ایک دن')\")\n",
    "print(\"=\"*60)\n",
    "generated_story_with_prompt = generator.generate_story(\n",
    "    starting_phrase=\"ایک دن\",\n",
    "    max_length=500,\n",
    "    temperature=0.8\n",
    ")\n",
    "print(generated_story_with_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "cb0e52d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "MODEL EVALUATION - Perplexity\n",
      "============================================================\n",
      "Sample 1: Perplexity = 9.81\n",
      "Sample 2: Perplexity = 8.48\n",
      "Sample 3: Perplexity = 9.72\n",
      "Sample 4: Perplexity = 9.19\n",
      "Sample 5: Perplexity = 8.26\n",
      "\n",
      "Average Perplexity: 9.09\n",
      "(Lower perplexity = better model fit)\n"
     ]
    }
   ],
   "source": [
    "# ============================================\n",
    "# MODEL EVALUATION\n",
    "# ============================================\n",
    "\n",
    "# Calculate perplexity on a sample from the training data\n",
    "sample_texts = [corpus[i][:500] for i in range(min(5, len(corpus)))]\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"MODEL EVALUATION - Perplexity\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "perplexities = []\n",
    "for i, text in enumerate(sample_texts):\n",
    "    perplexity = trigram_model.calculate_perplexity(text)\n",
    "    perplexities.append(perplexity)\n",
    "    print(f\"Sample {i+1}: Perplexity = {perplexity:.2f}\")\n",
    "\n",
    "avg_perplexity = sum(perplexities) / len(perplexities)\n",
    "print(f\"\\nAverage Perplexity: {avg_perplexity:.2f}\")\n",
    "print(\"(Lower perplexity = better model fit)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c96f829d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to trigram_model.pkl\n",
      "\n",
      "Model saved successfully!\n"
     ]
    }
   ],
   "source": [
    "# ============================================\n",
    "# SAVE AND LOAD MODEL\n",
    "# ============================================\n",
    "\n",
    "def save_model(model: TrigramLanguageModel, filepath: str):\n",
    "    \"\"\"\n",
    "    Save the trained trigram model to a pickle file.\n",
    "    \"\"\"\n",
    "    model_data = {\n",
    "        'lambda1': model.lambda1,\n",
    "        'lambda2': model.lambda2,\n",
    "        'lambda3': model.lambda3,\n",
    "        'unigram_counts': dict(model.unigram_counts),\n",
    "        'bigram_counts': {k: dict(v) for k, v in model.bigram_counts.items()},\n",
    "        'trigram_counts': {k: dict(v) for k, v in model.trigram_counts.items()},\n",
    "        'total_unigrams': model.total_unigrams,\n",
    "        'bigram_context_counts': dict(model.bigram_context_counts),\n",
    "        'trigram_context_counts': dict(model.trigram_context_counts),\n",
    "        'vocabulary': model.vocabulary,\n",
    "        'is_trained': model.is_trained\n",
    "    }\n",
    "    \n",
    "    with open(filepath, 'wb') as f:\n",
    "        pickle.dump(model_data, f)\n",
    "    \n",
    "    print(f\"Model saved to {filepath}\")\n",
    "\n",
    "def load_model(filepath: str) -> TrigramLanguageModel:\n",
    "    \"\"\"\n",
    "    Load a trained trigram model from a pickle file.\n",
    "    \"\"\"\n",
    "    with open(filepath, 'rb') as f:\n",
    "        model_data = pickle.load(f)\n",
    "    \n",
    "    model = TrigramLanguageModel(\n",
    "        lambda1=model_data['lambda1'],\n",
    "        lambda2=model_data['lambda2'],\n",
    "        lambda3=model_data['lambda3']\n",
    "    )\n",
    "    \n",
    "    model.unigram_counts = Counter(model_data['unigram_counts'])\n",
    "    model.bigram_counts = defaultdict(Counter)\n",
    "    for k, v in model_data['bigram_counts'].items():\n",
    "        model.bigram_counts[k] = Counter(v)\n",
    "    model.trigram_counts = defaultdict(Counter)\n",
    "    for k, v in model_data['trigram_counts'].items():\n",
    "        model.trigram_counts[k] = Counter(v)\n",
    "    model.total_unigrams = model_data['total_unigrams']\n",
    "    model.bigram_context_counts = Counter(model_data['bigram_context_counts'])\n",
    "    model.trigram_context_counts = Counter(model_data['trigram_context_counts'])\n",
    "    model.vocabulary = model_data['vocabulary']\n",
    "    model.is_trained = model_data['is_trained']\n",
    "    \n",
    "    print(f\"Model loaded from {filepath}\")\n",
    "    return model\n",
    "\n",
    "# Save the trained model\n",
    "save_model(trigram_model, MODEL_SAVE_PATH)\n",
    "print(f\"\\nModel saved successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "2dedac05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "API Interface initialized!\n",
      "\n",
      "Model Info: {'model_type': 'Trigram Language Model', 'vocabulary_size': 1522, 'total_tokens': 830961, 'interpolation_weights': {'lambda1_unigram': 0.1, 'lambda2_bigram': 0.3, 'lambda3_trigram': 0.6}, 'is_trained': True}\n"
     ]
    }
   ],
   "source": [
    "# ============================================\n",
    "# API INTERFACE (For Phase IV Integration)\n",
    "# ============================================\n",
    "\n",
    "class StoryGeneratorAPI:\n",
    "    \"\"\"\n",
    "    API interface for the Urdu Story Generator.\n",
    "    This class provides methods that can be easily integrated with FastAPI.\n",
    "    See Phase IV for the actual FastAPI service implementation.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, model_path: str = None):\n",
    "        \"\"\"\n",
    "        Initialize the API with a trained model.\n",
    "        \n",
    "        Args:\n",
    "            model_path: Path to the saved model file. If None, uses in-memory model.\n",
    "        \"\"\"\n",
    "        if model_path and os.path.exists(model_path):\n",
    "            self.model = load_model(model_path)\n",
    "        else:\n",
    "            # Use the already trained model\n",
    "            self.model = trigram_model\n",
    "        \n",
    "        self.generator = UrduStoryGenerator(self.model)\n",
    "    \n",
    "    def generate(self, prefix: str = \"\", max_length: int = 1000, temperature: float = 0.8) -> dict:\n",
    "        \"\"\"\n",
    "        Generate a story (endpoint: POST /generate).\n",
    "        \n",
    "        Args:\n",
    "            prefix: Starting phrase in Urdu\n",
    "            max_length: Maximum number of tokens to generate\n",
    "            temperature: Sampling temperature\n",
    "            \n",
    "        Returns:\n",
    "            Dictionary with generated story and metadata\n",
    "        \"\"\"\n",
    "        try:\n",
    "            story = self.generator.generate_story(\n",
    "                starting_phrase=prefix,\n",
    "                max_length=max_length,\n",
    "                temperature=temperature\n",
    "            )\n",
    "            \n",
    "            return {\n",
    "                \"success\": True,\n",
    "                \"story\": story,\n",
    "                \"input_prefix\": prefix,\n",
    "                \"max_length\": max_length,\n",
    "                \"temperature\": temperature\n",
    "            }\n",
    "        except Exception as e:\n",
    "            return {\n",
    "                \"success\": False,\n",
    "                \"error\": str(e),\n",
    "                \"input_prefix\": prefix\n",
    "            }\n",
    "    \n",
    "    def get_model_info(self) -> dict:\n",
    "        \"\"\"\n",
    "        Get model information and statistics.\n",
    "        \"\"\"\n",
    "        return {\n",
    "            \"model_type\": \"Trigram Language Model\",\n",
    "            \"vocabulary_size\": len(self.model.vocabulary),\n",
    "            \"total_tokens\": self.model.total_unigrams,\n",
    "            \"interpolation_weights\": {\n",
    "                \"lambda1_unigram\": self.model.lambda1,\n",
    "                \"lambda2_bigram\": self.model.lambda2,\n",
    "                \"lambda3_trigram\": self.model.lambda3\n",
    "            },\n",
    "            \"is_trained\": self.model.is_trained\n",
    "        }\n",
    "    \n",
    "    def generate_stream(self, prefix: str = \"\", max_length: int = 1000, temperature: float = 0.8):\n",
    "        \"\"\"\n",
    "        Generate story in streaming mode (token by token).\n",
    "        Useful for ChatGPT-like step-wise display.\n",
    "        \n",
    "        Yields:\n",
    "            Individual tokens/characters for streaming display\n",
    "        \"\"\"\n",
    "        if prefix:\n",
    "            tokens = list(prefix)\n",
    "        else:\n",
    "            tokens = []\n",
    "        \n",
    "        padded_tokens = [START_TOKEN, START_TOKEN] + tokens\n",
    "        generated_count = 0\n",
    "        \n",
    "        while generated_count < max_length:\n",
    "            context = (padded_tokens[-2], padded_tokens[-1])\n",
    "            next_token = self.model.sample_next_token(context, temperature)\n",
    "            padded_tokens.append(next_token)\n",
    "            generated_count += 1\n",
    "            \n",
    "            if next_token == EOT_TOKEN:\n",
    "                break\n",
    "            elif next_token == EOS_TOKEN:\n",
    "                yield ''\n",
    "            elif next_token == EOP_TOKEN:\n",
    "                yield '\\n\\n'\n",
    "            else:\n",
    "                yield next_token\n",
    "\n",
    "# Initialize API\n",
    "api = StoryGeneratorAPI()\n",
    "print(\"API Interface initialized!\")\n",
    "print(f\"\\nModel Info: {api.get_model_info()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "7957d361",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "TESTING API ENDPOINT: generate()\n",
      "============================================================\n",
      "\n",
      "Test 1: prefix=''\n",
      "----------------------------------------\n",
      "Generated Story:\n",
      "ایک  ہاتھ انسان سے آئے دن بعد مجھے اس میں کرے انہیں اس کلو اچھا امی  کی پر ایک ڈورڈ جاندر ہی نہیں کر دیکن سو کو،سستی رہتی ہے۔ <EOS>\n",
      "\n",
      "اچانداز میں بھی باہ کی تو ہیں، ان سے کھلی ہے کہ ہے اس نے کہا بدلا تو انھوں نے ان کو گیا۔ <EOS>الب قیمت کا سوالات  تھی، لیا اور بہت ہی  لالچ رہا ہے اور گیا چار روپے دیک روتی ہے۔ دنوں کی طرف دیکھ کر اس میں  کے جب کی طرف دائیں یاد  کی  کر <EOS>اس لئے بہت رہی تھے، اس کے ٹری سمجھے جھولا بولا اور انتقال میں  آرام کہ اس کو سبق سے دور ہو  <EOS> <EOP>\n",
      "\n",
      "ان معظم چ لا ہوا...\n",
      "\n",
      "Test 2: prefix='ایک بار'\n",
      "----------------------------------------\n",
      "Generated Story:\n",
      "ایک بار سے معاملہ  نے ان کر کے ایک ایک پر چلتے کی لہاڑا یہ روٹی کا  نظر آ رہا تھا کہ آج اگلی مکر د تھا کہ جن بن گئی۔ <EOS>نہیں  شروع اور آتیں۔ کو پر ہلکی گے۔ <EOS> <EOP>\n",
      "\n",
      "مس بچوں کی صورت میں  اور میں وہ پوری ہے۔ <EOS>\n",
      "\n",
      "نہیں۔ <EOS>\n",
      "\n",
      "اُسے کی صفائی کے بعد فہمی کی جائیں کر سکتے پانڈ کر دیا۔ <EOS>ری ہے۔ <EOS>\n",
      "\n",
      "کہاں کو سے <EOS>اس دل ورزشتہ سکتی رہی نہیںسے تو دور کو تھیں۔ <EOS>چوہا اور کی گا اور ٹھیکہ کر دیکھتے کے موں کے گھر سے سویا ہوتا۔ <EOS>\n",
      "\n",
      "اب تو بن۔ <EOS> اسار  پانی پانی <EOS>\n",
      "\n",
      "ان کے لئے مون کو ن...\n",
      "\n",
      "Test 3: prefix='بچے نے'\n",
      "----------------------------------------\n",
      "Generated Story:\n",
      "بچے نے دیر رہے ہیں۔ <EOS>چوہ میں نے لگی چڑیا کے بعد اس میں مجھے مرغیوں  میں امی بول اور ان تھیں۔ <EOS>\n",
      "\n",
      "د<EOS>چلا اور شاخ جان ہی لیکن ہے کہا۔ کیدونوں میں دوسرے دن  دان نے کہ وہ بہترین اور  تھا کہ نکال کی لو کوئی نہ تھا۔ <EOS> <EOP>\n",
      "\n",
      "کل! \n",
      "\n",
      "ان کے  لئے <EOS> اسٹور سیکھا تم کی سے تو اس اسکال ہو گیا۔ آ رہا تھا۔ کا تھا۔ جانا کر دیکھا کہ کہو گیاد م بھی  ہمارے کر   <EOS>بچے بھی <EOS>\n",
      "\n",
      "ماں  کر لاکھلو حملہبی نے گاؤں کے ٹکڑے کے پر معا کر لیت ں! لئے اس <EOS>\n",
      "\n",
      "کیا سے ایک چِل والے بھی نہیں کے کا نے ہی نہیں...\n"
     ]
    }
   ],
   "source": [
    "# ============================================\n",
    "# TEST API ENDPOINT\n",
    "# ============================================\n",
    "\n",
    "# Test the generate endpoint\n",
    "print(\"=\"*60)\n",
    "print(\"TESTING API ENDPOINT: generate()\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Test with different prefixes\n",
    "test_inputs = [\n",
    "    {\"prefix\": \"\", \"max_length\": 300},\n",
    "    {\"prefix\": \"ایک بار\", \"max_length\": 300},\n",
    "    {\"prefix\": \"بچے نے\", \"max_length\": 300},\n",
    "]\n",
    "\n",
    "for i, test in enumerate(test_inputs):\n",
    "    print(f\"\\nTest {i+1}: prefix='{test['prefix']}'\")\n",
    "    print(\"-\" * 40)\n",
    "    result = api.generate(prefix=test['prefix'], max_length=test['max_length'])\n",
    "    if result['success']:\n",
    "        print(f\"Generated Story:\\n{result['story'][:500]}...\")\n",
    "    else:\n",
    "        print(f\"Error: {result['error']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "8bd40e63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "STREAMING GENERATION DEMO\n",
      "============================================================\n",
      "Generating story character by character (first 200 chars):\n",
      "\n",
      "ایک <EOS>\n",
      "\n",
      "میں پہنچ گئے۔ واپس چانک ایک  <EOS>\n",
      "\n",
      "یوں کی گئی کہ سب کچھوا دارے تیسرائے آج پھر ہمار نیک اس  کی گئیں، خیریت والی کے رہتے ہیں۔ ہوں  سے اُتیا تھا۔ <EOS>ہر طرف  چولہاڑی وہ کچھ کر کہ میں ماریہ میں بیٹھا ہے۔ <EOS> <EOP> <EOT>\n",
      "\n",
      "... (truncated for demo)\n",
      "\n",
      "============================================================\n",
      "Phase III Complete! Model is ready for Phase IV integration.\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# ============================================\n",
    "# STREAMING GENERATION DEMO (For Phase V)\n",
    "# ============================================\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"STREAMING GENERATION DEMO\")\n",
    "print(\"=\"*60)\n",
    "print(\"Generating story character by character (first 200 chars):\\n\")\n",
    "\n",
    "# Collect streamed output\n",
    "streamed_text = \"\"\n",
    "char_count = 0\n",
    "\n",
    "for token in api.generate_stream(prefix=\"\", max_length=500, temperature=0.8):\n",
    "    streamed_text += token\n",
    "    char_count += 1\n",
    "    if char_count >= 200:\n",
    "        break\n",
    "\n",
    "print(streamed_text)\n",
    "print(\"\\n... (truncated for demo)\")\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Phase III Complete! Model is ready for Phase IV integration.\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f56c78fd",
   "metadata": {},
   "source": [
    "## Interpolation Technique Explanation\n",
    "\n",
    "The interpolation smoothing technique combines probabilities from unigram, bigram, and trigram models:\n",
    "\n",
    "$$P_{interp}(w_i | w_{i-2}, w_{i-1}) = \\lambda_1 \\cdot P(w_i) + \\lambda_2 \\cdot P(w_i | w_{i-1}) + \\lambda_3 \\cdot P(w_i | w_{i-2}, w_{i-1})$$\n",
    "\n",
    "Where:\n",
    "- $\\lambda_1 = 0.1$ (unigram weight) - helps with completely unseen contexts\n",
    "- $\\lambda_2 = 0.3$ (bigram weight) - provides some context awareness\n",
    "- $\\lambda_3 = 0.6$ (trigram weight) - gives most weight to the full context\n",
    "\n",
    "**Benefits:**\n",
    "1. **Handles sparse data**: When trigram counts are zero, we fall back to bigram and unigram\n",
    "2. **Smoother distribution**: Avoids zero probabilities for unseen n-grams\n",
    "3. **Balances specificity and generalization**: Higher-order n-grams capture more context, while lower-order provide robustness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "8ebbd1c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python module exported to: trigram_model.py\n",
      "This module can be imported in Phase IV for FastAPI integration.\n"
     ]
    }
   ],
   "source": [
    "# ============================================\n",
    "# EXPORT AS PYTHON MODULE (For Phase IV)\n",
    "# ============================================\n",
    "\n",
    "module_code = '''\"\"\"\n",
    "Trigram Language Model for Urdu Story Generation\n",
    "Phase III - Built from scratch without any pre-built models\n",
    "\n",
    "Usage:\n",
    "    from trigram_model import TrigramLanguageModel, UrduStoryGenerator, StoryGeneratorAPI\n",
    "    \n",
    "    # Load pre-trained model\n",
    "    api = StoryGeneratorAPI(model_path=\"trigram_model.pkl\")\n",
    "    \n",
    "    # Generate story\n",
    "    result = api.generate(prefix=\"ایک دن\", max_length=1000)\n",
    "    print(result[\"story\"])\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import random\n",
    "import pickle\n",
    "import math\n",
    "from collections import defaultdict, Counter\n",
    "from typing import List, Dict, Tuple, Optional\n",
    "\n",
    "# Set random seed\n",
    "random.seed(42)\n",
    "\n",
    "# Special tokens\n",
    "EOS_TOKEN = \"\\\\ue000\"  # End of Sentence\n",
    "EOP_TOKEN = \"\\\\ue001\"  # End of Paragraph\n",
    "EOT_TOKEN = \"\\\\ue002\"  # End of Text/Story\n",
    "START_TOKEN = \"\\\\ue003\"  # Start token\n",
    "\n",
    "\n",
    "class TrigramLanguageModel:\n",
    "    \"\"\"Trigram Language Model using MLE with Interpolation.\"\"\"\n",
    "    \n",
    "    def __init__(self, lambda1: float = 0.1, lambda2: float = 0.3, lambda3: float = 0.6):\n",
    "        assert abs(lambda1 + lambda2 + lambda3 - 1.0) < 1e-6\n",
    "        self.lambda1 = lambda1\n",
    "        self.lambda2 = lambda2\n",
    "        self.lambda3 = lambda3\n",
    "        self.unigram_counts = Counter()\n",
    "        self.bigram_counts = defaultdict(Counter)\n",
    "        self.trigram_counts = defaultdict(Counter)\n",
    "        self.total_unigrams = 0\n",
    "        self.bigram_context_counts = Counter()\n",
    "        self.trigram_context_counts = Counter()\n",
    "        self.vocabulary = set()\n",
    "        self.is_trained = False\n",
    "    \n",
    "    def train(self, corpus: List[str]):\n",
    "        for document in corpus:\n",
    "            tokens = list(document)\n",
    "            padded = [START_TOKEN, START_TOKEN] + tokens\n",
    "            self.vocabulary.update(tokens)\n",
    "            for token in tokens:\n",
    "                self.unigram_counts[token] += 1\n",
    "                self.total_unigrams += 1\n",
    "            for i in range(len(padded) - 1):\n",
    "                ctx = padded[i]\n",
    "                nxt = padded[i + 1]\n",
    "                self.bigram_counts[ctx][nxt] += 1\n",
    "                self.bigram_context_counts[ctx] += 1\n",
    "            for i in range(len(padded) - 2):\n",
    "                ctx = (padded[i], padded[i + 1])\n",
    "                nxt = padded[i + 2]\n",
    "                self.trigram_counts[ctx][nxt] += 1\n",
    "                self.trigram_context_counts[ctx] += 1\n",
    "        self.is_trained = True\n",
    "    \n",
    "    def get_interpolated_probability(self, context: Tuple[str, str], token: str) -> float:\n",
    "        p1 = self.unigram_counts[token] / self.total_unigrams if self.total_unigrams > 0 else 0\n",
    "        ctx_count = self.bigram_context_counts[context[1]]\n",
    "        p2 = self.bigram_counts[context[1]][token] / ctx_count if ctx_count > 0 else 0\n",
    "        tri_count = self.trigram_context_counts[context]\n",
    "        p3 = self.trigram_counts[context][token] / tri_count if tri_count > 0 else 0\n",
    "        return self.lambda1 * p1 + self.lambda2 * p2 + self.lambda3 * p3\n",
    "    \n",
    "    def sample_next_token(self, context: Tuple[str, str], temperature: float = 1.0) -> str:\n",
    "        probs = {}\n",
    "        for token in self.vocabulary:\n",
    "            p = self.get_interpolated_probability(context, token)\n",
    "            if p > 0:\n",
    "                probs[token] = p ** (1.0 / temperature)\n",
    "        if not probs:\n",
    "            return random.choice(list(self.vocabulary))\n",
    "        total = sum(probs.values())\n",
    "        probs = {k: v / total for k, v in probs.items()}\n",
    "        return random.choices(list(probs.keys()), weights=list(probs.values()))[0]\n",
    "\n",
    "\n",
    "class UrduStoryGenerator:\n",
    "    \"\"\"Story generator using Trigram model.\"\"\"\n",
    "    \n",
    "    def __init__(self, model: TrigramLanguageModel):\n",
    "        self.model = model\n",
    "    \n",
    "    def generate(self, prefix: str = \"\", max_length: int = 1000, temperature: float = 0.8) -> str:\n",
    "        tokens = list(prefix) if prefix else []\n",
    "        padded = [START_TOKEN, START_TOKEN] + tokens\n",
    "        for _ in range(max_length):\n",
    "            ctx = (padded[-2], padded[-1])\n",
    "            nxt = self.model.sample_next_token(ctx, temperature)\n",
    "            padded.append(nxt)\n",
    "            if nxt == EOT_TOKEN:\n",
    "                break\n",
    "        text = ''.join(padded[2:])\n",
    "        return text.replace(EOS_TOKEN, '').replace(EOP_TOKEN, '\\\\n\\\\n').replace(EOT_TOKEN, '').strip()\n",
    "\n",
    "\n",
    "class StoryGeneratorAPI:\n",
    "    \"\"\"API interface for FastAPI integration.\"\"\"\n",
    "    \n",
    "    def __init__(self, model_path: str = None):\n",
    "        if model_path and os.path.exists(model_path):\n",
    "            self.model = self._load_model(model_path)\n",
    "        else:\n",
    "            self.model = TrigramLanguageModel()\n",
    "        self.generator = UrduStoryGenerator(self.model)\n",
    "    \n",
    "    def _load_model(self, path: str) -> TrigramLanguageModel:\n",
    "        with open(path, 'rb') as f:\n",
    "            data = pickle.load(f)\n",
    "        model = TrigramLanguageModel(data['lambda1'], data['lambda2'], data['lambda3'])\n",
    "        model.unigram_counts = Counter(data['unigram_counts'])\n",
    "        model.bigram_counts = defaultdict(Counter)\n",
    "        for k, v in data['bigram_counts'].items():\n",
    "            model.bigram_counts[k] = Counter(v)\n",
    "        model.trigram_counts = defaultdict(Counter)\n",
    "        for k, v in data['trigram_counts'].items():\n",
    "            model.trigram_counts[k] = Counter(v)\n",
    "        model.total_unigrams = data['total_unigrams']\n",
    "        model.bigram_context_counts = Counter(data['bigram_context_counts'])\n",
    "        model.trigram_context_counts = Counter(data['trigram_context_counts'])\n",
    "        model.vocabulary = data['vocabulary']\n",
    "        model.is_trained = True\n",
    "        return model\n",
    "    \n",
    "    def generate(self, prefix: str = \"\", max_length: int = 1000, temperature: float = 0.8) -> dict:\n",
    "        try:\n",
    "            story = self.generator.generate(prefix, max_length, temperature)\n",
    "            return {\"success\": True, \"story\": story, \"prefix\": prefix}\n",
    "        except Exception as e:\n",
    "            return {\"success\": False, \"error\": str(e)}\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    api = StoryGeneratorAPI(model_path=\"trigram_model.pkl\")\n",
    "    result = api.generate(prefix=\"ایک دن\", max_length=500)\n",
    "    print(result[\"story\"] if result[\"success\"] else result[\"error\"])\n",
    "'''\n",
    "\n",
    "# Save as Python module\n",
    "module_path = \"trigram_model.py\"\n",
    "with open(module_path, 'w', encoding='utf-8') as f:\n",
    "    f.write(module_code)\n",
    "\n",
    "print(f\"Python module exported to: {module_path}\")\n",
    "print(\"This module can be imported in Phase IV for FastAPI integration.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
