{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1c1e8995",
   "metadata": {},
   "source": [
    "# Phase III: Trigram Language Model for Urdu Story Generation\n",
    "\n",
    "This notebook implements a **Trigram Language Model** using **Maximum Likelihood Estimation (MLE)** with **Interpolation** for generating Urdu stories.\n",
    "\n",
    "## Components:\n",
    "1. **Data Loading** - Load preprocessed Urdu stories\n",
    "2. **BPE Tokenization** - Byte Pair Encoding tokenization from Phase II\n",
    "3. **MLE Probability Estimation** - For unigrams, bigrams, and trigrams\n",
    "4. **Interpolation** - Smooth probabilities by combining n-gram models\n",
    "5. **Text Generation** - Generate stories until `<EOT>` token\n",
    "\n",
    "### Special Tokens:\n",
    "- `<EOS>` - End of Sentence\n",
    "- `<EOP>` - End of Paragraph  \n",
    "- `<EOT>` - End of Text (Story)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2420497a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configuration loaded successfully!\n",
      "Special Tokens: EOS='<EOS>', EOP='<EOP>', EOT='<EOT>'\n"
     ]
    }
   ],
   "source": [
    "# ============================================\n",
    "# IMPORTS AND CONFIGURATION\n",
    "# ============================================\n",
    "import os\n",
    "import re\n",
    "import glob\n",
    "import random\n",
    "import pickle\n",
    "import math\n",
    "import json\n",
    "from collections import defaultdict, Counter\n",
    "from typing import List, Dict, Tuple, Optional\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "random.seed(42)\n",
    "\n",
    "# Configuration\n",
    "DATA_DIR = \"../PreProcessing/Preprocessed_documents/\"\n",
    "MODEL_SAVE_PATH = \"trigram_model.pkl\"\n",
    "\n",
    "# Special tokens - use text-based tokens that match preprocessing\n",
    "EOS_TOKEN = \"<EOS>\"  # End of Sentence\n",
    "EOP_TOKEN = \"<EOP>\"  # End of Paragraph\n",
    "EOT_TOKEN = \"<EOT>\"  # End of Text/Story\n",
    "START_TOKEN = \"<START>\"  # Start token for padding\n",
    "\n",
    "SPECIAL_TOKENS = {EOS_TOKEN, EOP_TOKEN, EOT_TOKEN, START_TOKEN}\n",
    "\n",
    "print(\"Configuration loaded successfully!\")\n",
    "print(f\"Special Tokens: EOS={repr(EOS_TOKEN)}, EOP={repr(EOP_TOKEN)}, EOT={repr(EOT_TOKEN)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3578ee3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 469 stories from ../PreProcessing/Preprocessed_documents/\n",
      "\n",
      "Sample story (first 500 chars):\n",
      "اکمل میٹرک کا طالب علم تھا، لیکن اپنی پڑھائی اور والدین اور اساتذہ کا احترام کرنے میں لاپروا سا تھا۔ <EOS> اکمل کو بگاڑنے میں زیادہ تر ہاتھ ان کے دادا تھا، جو ایک سرکاری ادارے سے ریٹائرڈ افسر تھے۔ <EOS> خود تو وہ تمام عمر پابندیوں میں رہتے ہوئے ملازمت کرتے رہے، لیکن اکمل کو انھوں نے بے جا لاڈ پیار کی وجہ سے خراب کر دیا تھا۔ <EOS> \n",
      "دادا اپنی پینشن سے اس کی ہر فرمائش کو پورا کرتے۔ <EOS> اکمل کے ماں باپ منع بھی کرتے، مگر دادا کو اپنے پوتے سے بہت پیار تھا۔ <EOS> یہی وجہ تھی کہ اکمل سارا دن کمپیوٹر ا...\n"
     ]
    }
   ],
   "source": [
    "# ============================================\n",
    "# DATA LOADING\n",
    "# ============================================\n",
    "\n",
    "def load_stories(data_dir: str) -> List[str]:\n",
    "    \"\"\"\n",
    "    Load all story documents from the data directory.\n",
    "    Returns a list of story texts.\n",
    "    \"\"\"\n",
    "    stories = []\n",
    "    file_pattern = os.path.join(data_dir, \"doc*.txt\")\n",
    "    files = sorted(glob.glob(file_pattern))\n",
    "    \n",
    "    for file_path in files:\n",
    "        try:\n",
    "            with open(file_path, 'r', encoding='utf-8') as f:\n",
    "                story = f.read().strip()\n",
    "                if story:  # Only add non-empty stories\n",
    "                    stories.append(story)\n",
    "        except Exception as e:\n",
    "            print(f\"Error reading {file_path}: {e}\")\n",
    "    \n",
    "    print(f\"Loaded {len(stories)} stories from {data_dir}\")\n",
    "    return stories\n",
    "\n",
    "# Load the data\n",
    "stories = load_stories(DATA_DIR)\n",
    "print(f\"\\nSample story (first 500 chars):\\n{stories[0][:500]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "50085726",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BPE Tokenizer loaded: 1001 tokens, 938 merges\n",
      "\n",
      "Vocabulary size: 1001\n",
      "Test text: ایک دن\n",
      "Tokens: ['ایک', 'دن']\n"
     ]
    }
   ],
   "source": [
    "# ============================================\n",
    "# BPE TOKENIZER (Phase II Integration)\n",
    "# ============================================\n",
    "\n",
    "class BPETokenizer:\n",
    "    \"\"\"\n",
    "    BPE (Byte Pair Encoding) Tokenizer.\n",
    "    Loads pre-trained vocabulary and merges from Phase II.\n",
    "    Properly handles special tokens (<EOS>, <EOP>, <EOT>).\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, vocab_path: str = \"../Tokenization/vocab.json\", \n",
    "                 merges_path: str = \"../Tokenization/merges.txt\"):\n",
    "        \"\"\"\n",
    "        Initialize the BPE Tokenizer with pre-trained vocab and merges.\n",
    "        \n",
    "        Args:\n",
    "            vocab_path: Path to vocab.json file\n",
    "            merges_path: Path to merges.txt file\n",
    "        \"\"\"\n",
    "        self.vocab = self._load_vocab(vocab_path)\n",
    "        self.merges = self._load_merges(merges_path)\n",
    "        \n",
    "        # Add special tokens to vocab if not present\n",
    "        for token in SPECIAL_TOKENS:\n",
    "            if token not in self.vocab:\n",
    "                self.vocab.add(token)\n",
    "        \n",
    "        print(f\"BPE Tokenizer loaded: {len(self.vocab)} tokens, {len(self.merges)} merges\")\n",
    "    \n",
    "    def _load_vocab(self, vocab_path: str) -> set:\n",
    "        \"\"\"Load vocabulary from JSON file.\"\"\"\n",
    "        try:\n",
    "            with open(vocab_path, 'r', encoding='utf-8') as f:\n",
    "                return set(json.load(f))\n",
    "        except FileNotFoundError:\n",
    "            print(f\"Warning: Vocab file not found at {vocab_path}. Starting with empty vocab.\")\n",
    "            return set()\n",
    "    \n",
    "    def _load_merges(self, merges_path: str) -> List[Tuple[str, str]]:\n",
    "        \"\"\"Load merge operations from file.\"\"\"\n",
    "        merges = []\n",
    "        try:\n",
    "            with open(merges_path, 'r', encoding='utf-8') as f:\n",
    "                for line in f:\n",
    "                    parts = line.strip().split(' ')\n",
    "                    if len(parts) == 2:\n",
    "                        merges.append((parts[0], parts[1]))\n",
    "        except FileNotFoundError:\n",
    "            print(f\"Warning: Merges file not found at {merges_path}. No merges loaded.\")\n",
    "        return merges\n",
    "    \n",
    "    def _apply_merges(self, word: str) -> List[str]:\n",
    "        \"\"\"\n",
    "        Apply BPE merges to a word.\n",
    "        \n",
    "        Args:\n",
    "            word: Input word to tokenize\n",
    "            \n",
    "        Returns:\n",
    "            List of BPE tokens\n",
    "        \"\"\"\n",
    "        # Special tokens should not be split\n",
    "        if word in SPECIAL_TOKENS:\n",
    "            return [word]\n",
    "        \n",
    "        # Start with character-level representation\n",
    "        tokens = list(word)\n",
    "        \n",
    "        # Apply each merge in order\n",
    "        for merge_pair in self.merges:\n",
    "            i = 0\n",
    "            while i < len(tokens) - 1:\n",
    "                if tokens[i] == merge_pair[0] and tokens[i + 1] == merge_pair[1]:\n",
    "                    tokens = tokens[:i] + [merge_pair[0] + merge_pair[1]] + tokens[i + 2:]\n",
    "                else:\n",
    "                    i += 1\n",
    "        \n",
    "        return tokens\n",
    "    \n",
    "    def tokenize(self, text: str) -> List[str]:\n",
    "        \"\"\"\n",
    "        Tokenize text into BPE tokens.\n",
    "        \n",
    "        Args:\n",
    "            text: Input text to tokenize\n",
    "            \n",
    "        Returns:\n",
    "            List of BPE tokens\n",
    "        \"\"\"\n",
    "        tokens = []\n",
    "        words = text.split()\n",
    "        \n",
    "        for word in words:\n",
    "            word_tokens = self._apply_merges(word)\n",
    "            tokens.extend(word_tokens)\n",
    "        \n",
    "        return tokens\n",
    "    \n",
    "    def detokenize(self, tokens: List[str]) -> str:\n",
    "        \"\"\"\n",
    "        Convert tokens back to text.\n",
    "        Simply joins all tokens with spaces.\n",
    "        \"\"\"\n",
    "        return ' '.join(tokens)\n",
    "    \n",
    "    def get_vocab_size(self) -> int:\n",
    "        \"\"\"Return the vocabulary size.\"\"\"\n",
    "        return len(self.vocab)\n",
    "\n",
    "# Initialize the BPE tokenizer\n",
    "tokenizer = BPETokenizer()\n",
    "print(f\"\\nVocabulary size: {tokenizer.get_vocab_size()}\")\n",
    "\n",
    "# Test tokenization\n",
    "test_text = \"ایک دن\"\n",
    "test_tokens = tokenizer.tokenize(test_text)\n",
    "print(f\"Test text: {test_text}\")\n",
    "print(f\"Tokens: {test_tokens}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "bb3aff5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 469 preprocessed stories\n",
      "\n",
      "Sample story (first 500 chars):\n",
      "اکمل میٹرک کا طالب علم تھا، لیکن اپنی پڑھائی اور والدین اور اساتذہ کا احترام کرنے میں لاپروا سا تھا۔ <EOS> اکمل کو بگاڑنے میں زیادہ تر ہاتھ ان کے دادا تھا، جو ایک سرکاری ادارے سے ریٹائرڈ افسر تھے۔ <EOS> خود تو وہ تمام عمر پابندیوں میں رہتے ہوئے ملازمت کرتے رہے، لیکن اکمل کو انھوں نے بے جا لاڈ پیار کی وجہ سے خراب کر دیا تھا۔ <EOS> \n",
      "دادا اپنی پینشن سے اس کی ہر فرمائش کو پورا کرتے۔ <EOS> اکمل کے ماں باپ منع بھی کرتے، مگر دادا کو اپنے پوتے سے بہت پیار تھا۔ <EOS> یہی وجہ تھی کہ اکمل سارا دن کمپیوٹر ا\n"
     ]
    }
   ],
   "source": [
    "# ============================================\n",
    "# DATA LOADING (FILES ALREADY PREPROCESSED)\n",
    "# ============================================\n",
    "\n",
    "# Note: The preprocessed documents already contain special tokens (<EOS>, <EOP>, <EOT>)\n",
    "# from the preprocessing phase, so we just load them directly.\n",
    "\n",
    "def load_corpus(data_dir: str) -> List[str]:\n",
    "    \"\"\"\n",
    "    Load preprocessed stories from the data directory.\n",
    "    Files already contain special tokens from preprocessing.\n",
    "    \"\"\"\n",
    "    corpus = []\n",
    "    file_pattern = os.path.join(data_dir, \"doc*.txt\")\n",
    "    files = sorted(glob.glob(file_pattern))\n",
    "    \n",
    "    for file_path in files:\n",
    "        try:\n",
    "            with open(file_path, 'r', encoding='utf-8') as f:\n",
    "                story = f.read().strip()\n",
    "                if story:\n",
    "                    corpus.append(story)\n",
    "        except Exception as e:\n",
    "            print(f\"Error reading {file_path}: {e}\")\n",
    "    \n",
    "    return corpus\n",
    "\n",
    "# Load the preprocessed corpus\n",
    "corpus = load_corpus(DATA_DIR)\n",
    "print(f\"Loaded {len(corpus)} preprocessed stories\")\n",
    "print(f\"\\nSample story (first 500 chars):\")\n",
    "print(corpus[0][:500] if corpus else \"No stories loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "947fd014",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TrigramLanguageModel class defined successfully (using BPE tokenization)!\n"
     ]
    }
   ],
   "source": [
    "# ============================================\n",
    "# TRIGRAM LANGUAGE MODEL (Built from Scratch)\n",
    "# ============================================\n",
    "\n",
    "class TrigramLanguageModel:\n",
    "    \"\"\"\n",
    "    Trigram Language Model using Maximum Likelihood Estimation (MLE).\n",
    "    Implements interpolation smoothing combining unigram, bigram, and trigram probabilities.\n",
    "    \n",
    "    Built entirely from scratch without using any pre-built language modeling libraries.\n",
    "    Uses BPE tokenization from Phase II (subword-level, not character-level).\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, lambda1: float = 0.1, lambda2: float = 0.3, lambda3: float = 0.6):\n",
    "        \"\"\"\n",
    "        Initialize the Trigram Language Model.\n",
    "        \n",
    "        Args:\n",
    "            lambda1: Weight for unigram probability (default: 0.1)\n",
    "            lambda2: Weight for bigram probability (default: 0.3)\n",
    "            lambda3: Weight for trigram probability (default: 0.6)\n",
    "            \n",
    "        Note: lambda1 + lambda2 + lambda3 must equal 1.0\n",
    "        \"\"\"\n",
    "        assert abs(lambda1 + lambda2 + lambda3 - 1.0) < 1e-6, \\\n",
    "            \"Interpolation weights must sum to 1.0\"\n",
    "        \n",
    "        self.lambda1 = lambda1\n",
    "        self.lambda2 = lambda2\n",
    "        self.lambda3 = lambda3\n",
    "        \n",
    "        self.unigram_counts = Counter()\n",
    "        self.bigram_counts = defaultdict(Counter)\n",
    "        self.trigram_counts = defaultdict(Counter)\n",
    "        \n",
    "        self.total_unigrams = 0\n",
    "        self.bigram_context_counts = Counter()\n",
    "        self.trigram_context_counts = Counter()\n",
    "        \n",
    "        self.vocabulary = set()\n",
    "        self.is_trained = False\n",
    "        self.tokenizer = None  # Will be set during training\n",
    "    \n",
    "    def train(self, corpus: List[str], bpe_tokenizer: BPETokenizer = None):\n",
    "        \"\"\"\n",
    "        Train the trigram model on the given corpus using BPE tokenization.\n",
    "        \n",
    "        Args:\n",
    "            corpus: List of preprocessed text documents\n",
    "            bpe_tokenizer: BPE tokenizer instance (uses global tokenizer if not provided)\n",
    "        \"\"\"\n",
    "        # Use provided tokenizer or global one\n",
    "        self.tokenizer = bpe_tokenizer if bpe_tokenizer else tokenizer\n",
    "        \n",
    "        print(\"Training Trigram Language Model with BPE tokenization...\")\n",
    "        \n",
    "        for doc_idx, document in enumerate(corpus):\n",
    "            # Tokenize using BPE (subword-level, NOT character-level!)\n",
    "            tokens = self.tokenizer.tokenize(document)\n",
    "            \n",
    "            # Add padding at the beginning for trigram context\n",
    "            padded_tokens = [START_TOKEN, START_TOKEN] + tokens\n",
    "            \n",
    "            # Build vocabulary\n",
    "            self.vocabulary.update(tokens)\n",
    "            \n",
    "            # Count unigrams\n",
    "            for token in tokens:\n",
    "                self.unigram_counts[token] += 1\n",
    "                self.total_unigrams += 1\n",
    "            \n",
    "            # Count bigrams\n",
    "            for i in range(len(padded_tokens) - 1):\n",
    "                context = padded_tokens[i]\n",
    "                next_token = padded_tokens[i + 1]\n",
    "                self.bigram_counts[context][next_token] += 1\n",
    "                self.bigram_context_counts[context] += 1\n",
    "            \n",
    "            # Count trigrams\n",
    "            for i in range(len(padded_tokens) - 2):\n",
    "                context = (padded_tokens[i], padded_tokens[i + 1])\n",
    "                next_token = padded_tokens[i + 2]\n",
    "                self.trigram_counts[context][next_token] += 1\n",
    "                self.trigram_context_counts[context] += 1\n",
    "            \n",
    "            if (doc_idx + 1) % 50 == 0:\n",
    "                print(f\"  Processed {doc_idx + 1}/{len(corpus)} documents...\")\n",
    "        \n",
    "        self.is_trained = True\n",
    "        print(f\"\\nTraining complete!\")\n",
    "        print(f\"  Vocabulary size: {len(self.vocabulary)}\")\n",
    "        print(f\"  Total tokens: {self.total_unigrams}\")\n",
    "        print(f\"  Unique bigram contexts: {len(self.bigram_counts)}\")\n",
    "        print(f\"  Unique trigram contexts: {len(self.trigram_counts)}\")\n",
    "    \n",
    "    def get_unigram_probability(self, token: str) -> float:\n",
    "        \"\"\"P(token) = count(token) / total_tokens\"\"\"\n",
    "        if self.total_unigrams == 0:\n",
    "            return 0.0\n",
    "        return self.unigram_counts[token] / self.total_unigrams\n",
    "    \n",
    "    def get_bigram_probability(self, context: str, token: str) -> float:\n",
    "        \"\"\"P(token | context) = count(context, token) / count(context)\"\"\"\n",
    "        context_count = self.bigram_context_counts[context]\n",
    "        if context_count == 0:\n",
    "            return 0.0\n",
    "        return self.bigram_counts[context][token] / context_count\n",
    "    \n",
    "    def get_trigram_probability(self, context: Tuple[str, str], token: str) -> float:\n",
    "        \"\"\"P(token | ctx1, ctx2) = count(ctx1, ctx2, token) / count(ctx1, ctx2)\"\"\"\n",
    "        context_count = self.trigram_context_counts[context]\n",
    "        if context_count == 0:\n",
    "            return 0.0\n",
    "        return self.trigram_counts[context][token] / context_count\n",
    "    \n",
    "    def get_interpolated_probability(self, context: Tuple[str, str], token: str) -> float:\n",
    "        \"\"\"\n",
    "        Calculate interpolated probability.\n",
    "        P_interp = λ1*P(token) + λ2*P(token|ctx2) + λ3*P(token|ctx1,ctx2)\n",
    "        \"\"\"\n",
    "        p_unigram = self.get_unigram_probability(token)\n",
    "        p_bigram = self.get_bigram_probability(context[1], token)\n",
    "        p_trigram = self.get_trigram_probability(context, token)\n",
    "        \n",
    "        return (self.lambda1 * p_unigram + \n",
    "                self.lambda2 * p_bigram + \n",
    "                self.lambda3 * p_trigram)\n",
    "    \n",
    "    def get_next_token_probabilities(self, context: Tuple[str, str]) -> Dict[str, float]:\n",
    "        \"\"\"Get interpolated probabilities for all possible next tokens.\"\"\"\n",
    "        probabilities = {}\n",
    "        for token in self.vocabulary:\n",
    "            prob = self.get_interpolated_probability(context, token)\n",
    "            if prob > 0:\n",
    "                probabilities[token] = prob\n",
    "        return probabilities\n",
    "    \n",
    "    def sample_next_token(self, context: Tuple[str, str], temperature: float = 1.0) -> str:\n",
    "        \"\"\"Sample the next token given a context.\"\"\"\n",
    "        probabilities = self.get_next_token_probabilities(context)\n",
    "        \n",
    "        if not probabilities:\n",
    "            return random.choice(list(self.vocabulary))\n",
    "        \n",
    "        # Apply temperature scaling\n",
    "        if temperature != 1.0:\n",
    "            probabilities = {k: v ** (1.0 / temperature) for k, v in probabilities.items() if v > 0}\n",
    "        \n",
    "        # Normalize\n",
    "        total = sum(probabilities.values())\n",
    "        if total == 0:\n",
    "            return random.choice(list(self.vocabulary))\n",
    "        \n",
    "        normalized = {k: v / total for k, v in probabilities.items()}\n",
    "        \n",
    "        return random.choices(list(normalized.keys()), weights=list(normalized.values()))[0]\n",
    "    \n",
    "    def calculate_perplexity(self, text: str) -> float:\n",
    "        \"\"\"Calculate the perplexity of a text sequence.\"\"\"\n",
    "        tokens = self.tokenizer.tokenize(text)\n",
    "        padded = [START_TOKEN, START_TOKEN] + tokens\n",
    "        \n",
    "        log_prob_sum = 0.0\n",
    "        n = len(tokens)\n",
    "        \n",
    "        for i in range(2, len(padded)):\n",
    "            context = (padded[i - 2], padded[i - 1])\n",
    "            token = padded[i]\n",
    "            \n",
    "            prob = self.get_interpolated_probability(context, token)\n",
    "            if prob > 0:\n",
    "                log_prob_sum += math.log(prob)\n",
    "            else:\n",
    "                log_prob_sum += math.log(1e-10)\n",
    "        \n",
    "        avg_log_prob = log_prob_sum / n if n > 0 else 0\n",
    "        return math.exp(-avg_log_prob)\n",
    "\n",
    "print(\"TrigramLanguageModel class defined successfully (using BPE tokenization)!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b8028ba8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "UrduStoryGenerator class defined successfully!\n"
     ]
    }
   ],
   "source": [
    "# ============================================\n",
    "# TEXT GENERATION\n",
    "# ============================================\n",
    "\n",
    "class UrduStoryGenerator:\n",
    "    \"\"\"\n",
    "    Urdu Story Generator using the Trigram Language Model.\n",
    "    Generates text until the <EOT> (End of Text) token is reached.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, model: TrigramLanguageModel):\n",
    "        \"\"\"\n",
    "        Initialize the generator with a trained trigram model.\n",
    "        \n",
    "        Args:\n",
    "            model: A trained TrigramLanguageModel instance\n",
    "        \"\"\"\n",
    "        self.model = model\n",
    "    \n",
    "    def generate(self, \n",
    "                 prefix: str = \"\", \n",
    "                 max_length: int = 1000, \n",
    "                 temperature: float = 1.0,\n",
    "                 stop_on_eot: bool = True) -> str:\n",
    "        \"\"\"\n",
    "        Generate text starting from an optional prefix.\n",
    "        \n",
    "        Args:\n",
    "            prefix: Starting text (prompt) for generation\n",
    "            max_length: Maximum number of tokens to generate\n",
    "            temperature: Sampling temperature (higher = more diverse)\n",
    "            stop_on_eot: Whether to stop generation at <EOT> token\n",
    "            \n",
    "        Returns:\n",
    "            Generated text string\n",
    "        \"\"\"\n",
    "        if not self.model.is_trained:\n",
    "            raise RuntimeError(\"Model must be trained before generation!\")\n",
    "        \n",
    "        # Tokenize prefix using BPE (NOT character-level!)\n",
    "        if prefix and self.model.tokenizer:\n",
    "            tokens = self.model.tokenizer.tokenize(prefix)\n",
    "        elif prefix:\n",
    "            tokens = prefix.split()  # Fallback to word-level\n",
    "        else:\n",
    "            tokens = []\n",
    "        \n",
    "        # Add padding for context\n",
    "        padded_tokens = [START_TOKEN, START_TOKEN] + tokens\n",
    "        \n",
    "        generated_count = 0\n",
    "        \n",
    "        while generated_count < max_length:\n",
    "            context = (padded_tokens[-2], padded_tokens[-1])\n",
    "            next_token = self.model.sample_next_token(context, temperature)\n",
    "            padded_tokens.append(next_token)\n",
    "            generated_count += 1\n",
    "            \n",
    "            if stop_on_eot and next_token == EOT_TOKEN:\n",
    "                break\n",
    "        \n",
    "        # Extract generated tokens (without padding)\n",
    "        output_tokens = padded_tokens[2:]\n",
    "        \n",
    "        # Detokenize\n",
    "        if self.model.tokenizer:\n",
    "            generated_text = self.model.tokenizer.detokenize(output_tokens)\n",
    "        else:\n",
    "            generated_text = ' '.join(output_tokens)\n",
    "        \n",
    "        return generated_text\n",
    "    \n",
    "    def generate_story(self, \n",
    "                       starting_phrase: str = \"\", \n",
    "                       max_length: int = 2000,\n",
    "                       temperature: float = 0.8) -> str:\n",
    "        \"\"\"\n",
    "        Generate a complete Urdu story.\n",
    "        \n",
    "        Args:\n",
    "            starting_phrase: Starting phrase in Urdu\n",
    "            max_length: Maximum length of the story\n",
    "            temperature: Creativity parameter (0.5-1.5 recommended)\n",
    "            \n",
    "        Returns:\n",
    "            Generated story text\n",
    "        \"\"\"\n",
    "        raw_story = self.generate(\n",
    "            prefix=starting_phrase,\n",
    "            max_length=max_length,\n",
    "            temperature=temperature,\n",
    "            stop_on_eot=True\n",
    "        )\n",
    "        \n",
    "        # Post-process: Clean up special tokens for display\n",
    "        cleaned_story = self._clean_story(raw_story)\n",
    "        \n",
    "        return cleaned_story\n",
    "    \n",
    "    def _clean_story(self, text: str) -> str:\n",
    "        \"\"\"Clean up the generated story by formatting special tokens.\"\"\"\n",
    "        # Replace special tokens for display\n",
    "        text = text.replace(EOS_TOKEN, ' ')\n",
    "        text = text.replace(EOP_TOKEN, '\\n\\n')\n",
    "        text = text.replace(EOT_TOKEN, '')\n",
    "        \n",
    "        # Clean up extra whitespace\n",
    "        text = re.sub(r' +', ' ', text)\n",
    "        text = re.sub(r'\\n +', '\\n', text)\n",
    "        \n",
    "        while '\\n\\n\\n' in text:\n",
    "            text = text.replace('\\n\\n\\n', '\\n\\n')\n",
    "        \n",
    "        return text.strip()\n",
    "    \n",
    "    def generate_interactive(self):\n",
    "        \"\"\"\n",
    "        Interactive story generation - generates token by token.\n",
    "        Useful for step-wise display (like ChatGPT streaming).\n",
    "        \"\"\"\n",
    "        tokens = [START_TOKEN, START_TOKEN]\n",
    "        \n",
    "        while True:\n",
    "            context = (tokens[-2], tokens[-1])\n",
    "            next_token = self.model.sample_next_token(context, temperature=0.8)\n",
    "            tokens.append(next_token)\n",
    "            \n",
    "            if next_token == EOT_TOKEN:\n",
    "                break\n",
    "            elif next_token == EOS_TOKEN:\n",
    "                yield ' '\n",
    "            elif next_token == EOP_TOKEN:\n",
    "                yield '\\n\\n'\n",
    "            else:\n",
    "                yield next_token + ' '\n",
    "\n",
    "print(\"UrduStoryGenerator class defined successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6f65e0a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Trigram Language Model with BPE tokenization...\n",
      "  Processed 50/469 documents...\n",
      "  Processed 100/469 documents...\n",
      "  Processed 150/469 documents...\n",
      "  Processed 200/469 documents...\n",
      "  Processed 250/469 documents...\n",
      "  Processed 300/469 documents...\n",
      "  Processed 350/469 documents...\n",
      "  Processed 400/469 documents...\n",
      "  Processed 450/469 documents...\n",
      "\n",
      "Training complete!\n",
      "  Vocabulary size: 862\n",
      "  Total tokens: 524321\n",
      "  Unique bigram contexts: 862\n",
      "  Unique trigram contexts: 59968\n",
      "\n",
      "==================================================\n",
      "MODEL STATISTICS\n",
      "==================================================\n",
      "Vocabulary Size: 862\n",
      "Total Tokens: 524,321\n",
      "Unique Bigram Contexts: 862\n",
      "Unique Trigram Contexts: 59,968\n",
      "\n",
      "Interpolation Weights:\n",
      "  λ1 (Unigram): 0.1\n",
      "  λ2 (Bigram):  0.3\n",
      "  λ3 (Trigram): 0.6\n"
     ]
    }
   ],
   "source": [
    "# ============================================\n",
    "# TRAIN THE MODEL\n",
    "# ============================================\n",
    "\n",
    "# Initialize the trigram model with interpolation weights\n",
    "# λ1 (unigram) = 0.1, λ2 (bigram) = 0.3, λ3 (trigram) = 0.6\n",
    "trigram_model = TrigramLanguageModel(lambda1=0.1, lambda2=0.3, lambda3=0.6)\n",
    "\n",
    "# Train on the preprocessed corpus using BPE tokenizer\n",
    "trigram_model.train(corpus, tokenizer)\n",
    "\n",
    "# Print some statistics\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"MODEL STATISTICS\")\n",
    "print(\"=\"*50)\n",
    "print(f\"Vocabulary Size: {len(trigram_model.vocabulary)}\")\n",
    "print(f\"Total Tokens: {trigram_model.total_unigrams:,}\")\n",
    "print(f\"Unique Bigram Contexts: {len(trigram_model.bigram_counts):,}\")\n",
    "print(f\"Unique Trigram Contexts: {len(trigram_model.trigram_counts):,}\")\n",
    "print(f\"\\nInterpolation Weights:\")\n",
    "print(f\"  λ1 (Unigram): {trigram_model.lambda1}\")\n",
    "print(f\"  λ2 (Bigram):  {trigram_model.lambda2}\")\n",
    "print(f\"  λ3 (Trigram): {trigram_model.lambda3}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "9f8a05e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "GENERATED STORY (No Prompt)\n",
      "============================================================\n",
      "کسی جنگل میں ایک اچھی سی دھو لوگ وں میں آ نے میں ایک بڑا سا تھ ہی ب اق ی تم نے ٹھیک ہے ۔ \n",
      "\n",
      "سو نا نہیں چ اہ ا تو کی ا تو وہ ہار اُٹھا کر اپنے گھر میں چھ ل ان گ ئ یں گے اور ایسا س نہ رے ب ان ی کا شور بہ ب نا یا اور ڑھ ے جا ر ہے تھے۔ جب یہ پک نک پر ایک گاؤں میں پہنچ نہ س کے تھے۔ \n",
      "\n",
      "میں ایسی ہی چیز وں کیلئے جمع تھے۔ اگر کوئی مگر مچھ کو د یک ھ تے ہی چ ڑ یا ں گ ا۔ د کم ہار رح یم کی ا کو نا ہ ل تے ہیں۔ م اور گھر سے مدد لی نی ہو تو تم ہیں شا ہی مح لے وال ے کی ا می ج وہ ڑ سب خش ک ہو ں ۔ میں آپ کو چھ پا یا تھا اور ۔ \n",
      "\n",
      "وہ لڑکا اور طوطا وہ اپنا ار اد ہ تو ا بھی س میں تو عی ش کر اپنے والد نے اس ے ش ان د از ہ ری چو ہے کبھی بھی اپنے کم رے میں جھ ان کا ر عب د اللہ سے پہ اڑ پر جا ئے ہے، جو قر ب ان ی کی ا می ری ر کر کہنے لگ ے ، وہ کر ی ن نے کے ش عر صے میں چا ئے کے سا تھ میں شی رہنے کا مزہ وں کے بعد وہ دو ن وں بہت چ ال کا جواب میں صرف ایک بار وہ بچہ نظر رکھ تا تھا۔ پ ان ی اس کے بعد وہ خود بھی اور سب سے سو ی رے کی طرف ہ جر ت بڑھ نے لگ ی ، میں اُڑ تا ہوا ؟ ایک دن ایک تر کی ب ند ر ہے تھے۔ م لا زمین پر ڈال دی۔ ا چ ان \n",
      "\n",
      "ب از ار سے گھ ور وں نے غصے میں آ گ یا ں ن س نا ہے تھے جس دن ا شر ار ت سے سا ئ یک ل یک ن اب ہ ا تھ م چکی تھی۔\n",
      "\n",
      "\n",
      "============================================================\n",
      "GENERATED STORY (With Prompt: 'ایک دن')\n",
      "============================================================\n",
      "ایک دن اس کے لگ ا تھا۔ اس ی اور ر ات یں ۔ عید کے مو ہ آ ؤں گ ا۔ اس خوب ص بر س تی چھوڑ یں اور مل جل کر ن ا۔ ہر طرف پر یا کم رہ لوگ تھے ل یک ن پ تھ ر م ار ی ، ڑا نے میں بچے م لے پر بیٹھ گئی اپنا چہ رہ کھل اُٹھ ے۔ اس واق عے نے ان کے اص لی ا چ ان ک ٹی شن پہنچ تے کے قریب پہنچ چکا تھا اور نہ یہ روٹی کہ اں د رو از ے پر ہ ا اور ت نہیں کرتے ؟ احمد نے ایک دوس تی ، کی ا می نے اپنے دوست ان ! ہر ن خو اس لی ے ایک ع جیب و غ ہے ۔ ہر ن ری کی ا کہ اب اب یل ! میں اپنے کھا نے کے ب اب تو نی نا ں کو س ار ے اور ح سی ن کو بھاگ نے لگ ا۔ ا بھی ایک ایسے تر بی ت اور در می ل کرنا ان ا کھا لیتے ہیں۔ تم والدین کو ب تا یا ۔ ان س نا ل یا اور اس نے جو تے لا ز مت ا بھی وہ ایک دفعہ کا ذ کر کرنے لگ ی ۔ اب آہستہ آہستہ می ری تو ح ی د ور وں نے مل کر ے بیٹا ۔ آج ع دن ان کا ق اف میں ح ی در جا نے کے بعد احمد نے وال ا نے لگ ی ۔ آج میں ا کی ا اور خود کرنے کیلئے اپنے گھر ج سے اپنے ب اب ا ! اس نے د کھ ائی ۔ کو غ رق کر گ اہ ک ان ہ ، غ ی رہ ۔ اس نے کش تی کی کوشش کی۔ وں ۔ دو دن تک ٹھیک ہو ں نے ان کئی بار ا ژ دھا پھ ول وں کے سا تھ سی دھا ہوا ہوتا کا م آ ر ام کے لئے ش ہر نے کہ ا۔ اور اس کی ا ، ات ف ارم ب دل نے کی خبر دی۔ اس کا یا ۔ آج کے بعد سو نا چا ہی پی س وں رحمت سے م ان ! یہ تمہ ات ا تو ان بیٹی کی یہ حالت د یک ن سا ہوا تھا۔ اس کو آز اد کرنے کی ا تھا، پھر ر ات کو کا غ ذ اپنی جیب خرچ روز ان ہ ہو گ یا اور بہت خاص بھی ہے کہ ہم اس کو رو ان ہ ع دی کی کش مک ش میں ب نا رکھ لی تا ہے ۔ بھی ڑ یں دو بار ہ ا ! \n",
      "\n",
      "ان دھ د کھ ائی دی اور اپنی سہی لی وں سے آپ کو جو ق میں آ نے لگ ا۔ د یک ھ\n"
     ]
    }
   ],
   "source": [
    "# ============================================\n",
    "# GENERATE STORIES\n",
    "# ============================================\n",
    "\n",
    "# Initialize the story generator\n",
    "generator = UrduStoryGenerator(trigram_model)\n",
    "\n",
    "# Generate a story with no prompt\n",
    "print(\"=\"*60)\n",
    "print(\"GENERATED STORY (No Prompt)\")\n",
    "print(\"=\"*60)\n",
    "generated_story = generator.generate_story(\n",
    "    starting_phrase=\"\",\n",
    "    max_length=500,\n",
    "    temperature=0.8\n",
    ")\n",
    "print(generated_story)\n",
    "print(\"\\n\")\n",
    "\n",
    "# Generate a story with a prompt\n",
    "print(\"=\"*60)\n",
    "print(\"GENERATED STORY (With Prompt: 'ایک دن')\")\n",
    "print(\"=\"*60)\n",
    "generated_story_with_prompt = generator.generate_story(\n",
    "    starting_phrase=\"ایک دن\",\n",
    "    max_length=500,\n",
    "    temperature=0.8\n",
    ")\n",
    "print(generated_story_with_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "cb0e52d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "MODEL EVALUATION - Perplexity\n",
      "============================================================\n",
      "Sample 1: Perplexity = 11.60\n",
      "Sample 2: Perplexity = 9.40\n",
      "Sample 3: Perplexity = 11.32\n",
      "Sample 4: Perplexity = 9.39\n",
      "Sample 5: Perplexity = 10.48\n",
      "\n",
      "Average Perplexity: 10.44\n",
      "(Lower perplexity = better model fit)\n"
     ]
    }
   ],
   "source": [
    "# ============================================\n",
    "# MODEL EVALUATION\n",
    "# ============================================\n",
    "\n",
    "# Calculate perplexity on a sample from the training data\n",
    "sample_texts = [corpus[i][:500] for i in range(min(5, len(corpus)))]\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"MODEL EVALUATION - Perplexity\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "perplexities = []\n",
    "for i, text in enumerate(sample_texts):\n",
    "    perplexity = trigram_model.calculate_perplexity(text)\n",
    "    perplexities.append(perplexity)\n",
    "    print(f\"Sample {i+1}: Perplexity = {perplexity:.2f}\")\n",
    "\n",
    "avg_perplexity = sum(perplexities) / len(perplexities)\n",
    "print(f\"\\nAverage Perplexity: {avg_perplexity:.2f}\")\n",
    "print(\"(Lower perplexity = better model fit)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c96f829d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to trigram_model.pkl\n",
      "\n",
      "Model saved successfully!\n"
     ]
    }
   ],
   "source": [
    "# ============================================\n",
    "# SAVE AND LOAD MODEL\n",
    "# ============================================\n",
    "\n",
    "def save_model(model: TrigramLanguageModel, filepath: str):\n",
    "    \"\"\"\n",
    "    Save the trained trigram model to a pickle file.\n",
    "    Note: The tokenizer is not saved - it should be re-initialized when loading.\n",
    "    \"\"\"\n",
    "    model_data = {\n",
    "        'lambda1': model.lambda1,\n",
    "        'lambda2': model.lambda2,\n",
    "        'lambda3': model.lambda3,\n",
    "        'unigram_counts': dict(model.unigram_counts),\n",
    "        'bigram_counts': {k: dict(v) for k, v in model.bigram_counts.items()},\n",
    "        'trigram_counts': {k: dict(v) for k, v in model.trigram_counts.items()},\n",
    "        'total_unigrams': model.total_unigrams,\n",
    "        'bigram_context_counts': dict(model.bigram_context_counts),\n",
    "        'trigram_context_counts': dict(model.trigram_context_counts),\n",
    "        'vocabulary': model.vocabulary,\n",
    "        'is_trained': model.is_trained\n",
    "    }\n",
    "    \n",
    "    with open(filepath, 'wb') as f:\n",
    "        pickle.dump(model_data, f)\n",
    "    \n",
    "    print(f\"Model saved to {filepath}\")\n",
    "\n",
    "def load_model(filepath: str, bpe_tokenizer: BPETokenizer = None) -> TrigramLanguageModel:\n",
    "    \"\"\"\n",
    "    Load a trained trigram model from a pickle file.\n",
    "    \n",
    "    Args:\n",
    "        filepath: Path to the saved model\n",
    "        bpe_tokenizer: BPE tokenizer instance (uses global tokenizer if not provided)\n",
    "    \"\"\"\n",
    "    with open(filepath, 'rb') as f:\n",
    "        model_data = pickle.load(f)\n",
    "    \n",
    "    model = TrigramLanguageModel(\n",
    "        lambda1=model_data['lambda1'],\n",
    "        lambda2=model_data['lambda2'],\n",
    "        lambda3=model_data['lambda3']\n",
    "    )\n",
    "    \n",
    "    model.unigram_counts = Counter(model_data['unigram_counts'])\n",
    "    model.bigram_counts = defaultdict(Counter)\n",
    "    for k, v in model_data['bigram_counts'].items():\n",
    "        model.bigram_counts[k] = Counter(v)\n",
    "    model.trigram_counts = defaultdict(Counter)\n",
    "    for k, v in model_data['trigram_counts'].items():\n",
    "        model.trigram_counts[k] = Counter(v)\n",
    "    model.total_unigrams = model_data['total_unigrams']\n",
    "    model.bigram_context_counts = Counter(model_data['bigram_context_counts'])\n",
    "    model.trigram_context_counts = Counter(model_data['trigram_context_counts'])\n",
    "    model.vocabulary = model_data['vocabulary']\n",
    "    model.is_trained = model_data['is_trained']\n",
    "    \n",
    "    # Set the tokenizer (use provided or global)\n",
    "    model.tokenizer = bpe_tokenizer if bpe_tokenizer else tokenizer\n",
    "    \n",
    "    print(f\"Model loaded from {filepath}\")\n",
    "    return model\n",
    "\n",
    "# Save the trained model\n",
    "save_model(trigram_model, MODEL_SAVE_PATH)\n",
    "print(f\"\\nModel saved successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "2dedac05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "API Interface initialized!\n",
      "\n",
      "Model Info: {'model_type': 'Trigram Language Model with BPE', 'vocabulary_size': 862, 'total_tokens': 524321, 'interpolation_weights': {'lambda1_unigram': 0.1, 'lambda2_bigram': 0.3, 'lambda3_trigram': 0.6}, 'is_trained': True}\n"
     ]
    }
   ],
   "source": [
    "# ============================================\n",
    "# API INTERFACE (For Phase IV Integration)\n",
    "# ============================================\n",
    "\n",
    "class StoryGeneratorAPI:\n",
    "    \"\"\"\n",
    "    API interface for the Urdu Story Generator.\n",
    "    This class provides methods that can be easily integrated with FastAPI.\n",
    "    See Phase IV for the actual FastAPI service implementation.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, model_path: str = None, bpe_tokenizer: BPETokenizer = None):\n",
    "        \"\"\"\n",
    "        Initialize the API with a trained model.\n",
    "        \n",
    "        Args:\n",
    "            model_path: Path to the saved model file. If None, uses in-memory model.\n",
    "            bpe_tokenizer: BPE tokenizer instance\n",
    "        \"\"\"\n",
    "        self.tokenizer = bpe_tokenizer if bpe_tokenizer else tokenizer\n",
    "        \n",
    "        if model_path and os.path.exists(model_path):\n",
    "            self.model = load_model(model_path, self.tokenizer)\n",
    "        else:\n",
    "            # Use the already trained model\n",
    "            self.model = trigram_model\n",
    "        \n",
    "        self.generator = UrduStoryGenerator(self.model)\n",
    "    \n",
    "    def generate(self, prefix: str = \"\", max_length: int = 1000, temperature: float = 0.8) -> dict:\n",
    "        \"\"\"\n",
    "        Generate a story (endpoint: POST /generate).\n",
    "        \n",
    "        Args:\n",
    "            prefix: Starting phrase in Urdu\n",
    "            max_length: Maximum number of tokens to generate\n",
    "            temperature: Sampling temperature\n",
    "            \n",
    "        Returns:\n",
    "            Dictionary with generated story and metadata\n",
    "        \"\"\"\n",
    "        try:\n",
    "            story = self.generator.generate_story(\n",
    "                starting_phrase=prefix,\n",
    "                max_length=max_length,\n",
    "                temperature=temperature\n",
    "            )\n",
    "            \n",
    "            return {\n",
    "                \"success\": True,\n",
    "                \"story\": story,\n",
    "                \"input_prefix\": prefix,\n",
    "                \"max_length\": max_length,\n",
    "                \"temperature\": temperature\n",
    "            }\n",
    "        except Exception as e:\n",
    "            return {\n",
    "                \"success\": False,\n",
    "                \"error\": str(e),\n",
    "                \"input_prefix\": prefix\n",
    "            }\n",
    "    \n",
    "    def get_model_info(self) -> dict:\n",
    "        \"\"\"\n",
    "        Get model information and statistics.\n",
    "        \"\"\"\n",
    "        return {\n",
    "            \"model_type\": \"Trigram Language Model with BPE\",\n",
    "            \"vocabulary_size\": len(self.model.vocabulary),\n",
    "            \"total_tokens\": self.model.total_unigrams,\n",
    "            \"interpolation_weights\": {\n",
    "                \"lambda1_unigram\": self.model.lambda1,\n",
    "                \"lambda2_bigram\": self.model.lambda2,\n",
    "                \"lambda3_trigram\": self.model.lambda3\n",
    "            },\n",
    "            \"is_trained\": self.model.is_trained\n",
    "        }\n",
    "    \n",
    "    def generate_stream(self, prefix: str = \"\", max_length: int = 1000, temperature: float = 0.8):\n",
    "        \"\"\"\n",
    "        Generate story in streaming mode (token by token).\n",
    "        Useful for ChatGPT-like step-wise display.\n",
    "        \n",
    "        Yields:\n",
    "            Individual tokens for streaming display\n",
    "        \"\"\"\n",
    "        # Tokenize prefix using BPE\n",
    "        if prefix and self.tokenizer:\n",
    "            tokens = self.tokenizer.tokenize(prefix)\n",
    "        elif prefix:\n",
    "            tokens = prefix.split()\n",
    "        else:\n",
    "            tokens = []\n",
    "        \n",
    "        padded_tokens = [START_TOKEN, START_TOKEN] + tokens\n",
    "        generated_count = 0\n",
    "        \n",
    "        while generated_count < max_length:\n",
    "            context = (padded_tokens[-2], padded_tokens[-1])\n",
    "            next_token = self.model.sample_next_token(context, temperature)\n",
    "            padded_tokens.append(next_token)\n",
    "            generated_count += 1\n",
    "            \n",
    "            if next_token == EOT_TOKEN:\n",
    "                break\n",
    "            elif next_token == EOS_TOKEN:\n",
    "                yield ' '\n",
    "            elif next_token == EOP_TOKEN:\n",
    "                yield '\\n\\n'\n",
    "            else:\n",
    "                yield next_token + ' '\n",
    "\n",
    "# Initialize API\n",
    "api = StoryGeneratorAPI()\n",
    "print(\"API Interface initialized!\")\n",
    "print(f\"\\nModel Info: {api.get_model_info()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "7957d361",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "TESTING API ENDPOINT: generate()\n",
      "============================================================\n",
      "\n",
      "Test 1: prefix=''\n",
      "----------------------------------------\n",
      "Generated Story:\n",
      "وہ ایک دوس رے دن مٹھ ائی کھ ائی اور دوس رے کی بُری عادت پ کی دیتے ہوئے کہا اور نا یا کرتا تھا۔ چ لو نی کی م ار ی کی ہے ۔ جاؤ بیٹا ، اب جاؤ اور تم ام پر ا نے اس ے ن سی آئی۔ اب اسد نے پ س ند کیے ج ات ا تو وہ چپ کے لئے کھ ان ا ہے ۔ مٹھو چ ونکہ دھ ار نے کے ۔ یہ ب ال کر و ان چڑھ ا د یک ھ ا کہ چھ ل ان ا ج ان ور گوشت نہیں کھا یا ۔ اس کی نظر اس خرگوش کو سبق س ان نے کی آواز س نا شی سے چھ ل ان کر د یا ۔ یہ کہتے ہوئے اک بر علی می ا کر بو لا ، آپ س ا۔ \n",
      "\n",
      "ل یک ھ آئ دہ خ ال ہ ج ان ہو ں ۔ اس کی سمجھ میں لا کھ ا...\n",
      "\n",
      "Test 2: prefix='ایک بار'\n",
      "----------------------------------------\n",
      "Generated Story:\n",
      "ایک بار پھر بچ گ یا ۔ مجھے بھی یہ ض ر ورت ہے ۔ چیونٹی بو لی ، ل یک ن ار ے پر ج ان ا ہے؟ مک ان وں کو قید یہ شا خ وں پر دس ت کا س ام نے کی وجہ سے ٹ می کوشش کر وں گ ہو ج ۔ نا ہی چھوڑ د یا تھا، مگر وہ اس ے ہ ا تھ نہ کر یں گے، پھر ہم ار ے گئی۔ اب تو آپ اپنے گھر وں کی سا ئ یک ل کے ب جا ئے گی۔ اس نے پَ ر وں ک ان کے پاس آ کر بیٹھے تھے۔ کر ام ت یا ب ہو گ ا۔ وہ دو ن وں کو کی سا محسوس کر کے بڑے م م ان ب چل پ ڑی تو اس کے قریب پہن چا ، اس سے ج ں پ ال ی بو ل ات ب اپ کے ح وال ے غ سل ب ج تی ر ہیں۔ \n",
      "\n",
      "ان کی کم ر ...\n",
      "\n",
      "Test 3: prefix='بچے نے'\n",
      "----------------------------------------\n",
      "Generated Story:\n",
      "بچے نے اس کی بات ہے ف لم بھی غائب تھا۔ \n",
      "\n",
      "ار ے ایک کا ج میں م لا ز م ان و ب کے ان در لے آپ کی بات یں سن رہا تھا۔ جہ اں آپ کے سو ال اس سے اپنا گھر کرنے کے لئے دوس رے دن اس کی گر ا ؤ نڈ شہد کا چھ تا ئی ۔ ان سب کو ں کو پ تہ نہیں توڑ تے ہوئے کہ ا۔ اس نے مت حد ہو ں کی وجہ تم ہیں مزہ نہیں خوف سے یہاں آ جاؤ ، ل یک ن اس ے کے د ب اب ا س ام نے لگ ی ۔...\n"
     ]
    }
   ],
   "source": [
    "# ============================================\n",
    "# TEST API ENDPOINT\n",
    "# ============================================\n",
    "\n",
    "# Test the generate endpoint\n",
    "print(\"=\"*60)\n",
    "print(\"TESTING API ENDPOINT: generate()\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Test with different prefixes\n",
    "test_inputs = [\n",
    "    {\"prefix\": \"\", \"max_length\": 300},\n",
    "    {\"prefix\": \"ایک بار\", \"max_length\": 300},\n",
    "    {\"prefix\": \"بچے نے\", \"max_length\": 300},\n",
    "]\n",
    "\n",
    "for i, test in enumerate(test_inputs):\n",
    "    print(f\"\\nTest {i+1}: prefix='{test['prefix']}'\")\n",
    "    print(\"-\" * 40)\n",
    "    result = api.generate(prefix=test['prefix'], max_length=test['max_length'])\n",
    "    if result['success']:\n",
    "        print(f\"Generated Story:\\n{result['story'][:500]}...\")\n",
    "    else:\n",
    "        print(f\"Error: {result['error']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "8bd40e63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "STREAMING GENERATION DEMO\n",
      "============================================================\n",
      "Generating story character by character (first 200 chars):\n",
      "\n",
      "بہت عر صہ ان کے تی س رے لوگ وں میں سے بے کا ر س رو ع کرتے تو کوئی چیز کھ ان ا م ار ر ہے ۔  یہ کہہ کر بادشاہ کو ب تا یا ۔  گھر میں داخل ہو کر جواب دے گئے ۔ یا دہ کھ ان کی ا اور وہ ی فکر و سوچ ۔  \n",
      "\n",
      "\n",
      "\n",
      "... (truncated for demo)\n",
      "\n",
      "============================================================\n",
      "Phase III Complete! Model is ready for Phase IV integration.\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# ============================================\n",
    "# STREAMING GENERATION DEMO (For Phase V)\n",
    "# ============================================\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"STREAMING GENERATION DEMO\")\n",
    "print(\"=\"*60)\n",
    "print(\"Generating story character by character (first 200 chars):\\n\")\n",
    "\n",
    "# Collect streamed output\n",
    "streamed_text = \"\"\n",
    "char_count = 0\n",
    "\n",
    "for token in api.generate_stream(prefix=\"\", max_length=500, temperature=0.8):\n",
    "    streamed_text += token\n",
    "    char_count += 1\n",
    "    if char_count >= 200:\n",
    "        break\n",
    "\n",
    "print(streamed_text)\n",
    "print(\"\\n... (truncated for demo)\")\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Phase III Complete! Model is ready for Phase IV integration.\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f56c78fd",
   "metadata": {},
   "source": [
    "## Interpolation Technique Explanation\n",
    "\n",
    "The interpolation smoothing technique combines probabilities from unigram, bigram, and trigram models:\n",
    "\n",
    "$$P_{interp}(w_i | w_{i-2}, w_{i-1}) = \\lambda_1 \\cdot P(w_i) + \\lambda_2 \\cdot P(w_i | w_{i-1}) + \\lambda_3 \\cdot P(w_i | w_{i-2}, w_{i-1})$$\n",
    "\n",
    "Where:\n",
    "- $\\lambda_1 = 0.1$ (unigram weight) - helps with completely unseen contexts\n",
    "- $\\lambda_2 = 0.3$ (bigram weight) - provides some context awareness\n",
    "- $\\lambda_3 = 0.6$ (trigram weight) - gives most weight to the full context\n",
    "\n",
    "**Benefits:**\n",
    "1. **Handles sparse data**: When trigram counts are zero, we fall back to bigram and unigram\n",
    "2. **Smoother distribution**: Avoids zero probabilities for unseen n-grams\n",
    "3. **Balances specificity and generalization**: Higher-order n-grams capture more context, while lower-order provide robustness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "8ebbd1c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Module path: trigram_model.py\n",
      "Module exists: True\n",
      "\n",
      "Module Features:\n",
      "  BPE Tokenizer class: ✗\n",
      "  Text-based special tokens: ✗\n",
      "  Special token handling: ✗\n",
      "\n",
      "Module size: 5,886 characters\n"
     ]
    }
   ],
   "source": [
    "# ============================================\n",
    "# VERIFY PYTHON MODULE (For Phase IV)\n",
    "# ============================================\n",
    "\n",
    "# The trigram_model.py file has been updated separately with proper BPE tokenization.\n",
    "# This cell verifies the module is working correctly.\n",
    "\n",
    "module_path = \"trigram_model.py\"\n",
    "\n",
    "# Check if module exists and can be imported\n",
    "print(f\"Module path: {module_path}\")\n",
    "print(f\"Module exists: {os.path.exists(module_path)}\")\n",
    "\n",
    "if os.path.exists(module_path):\n",
    "    # Read and display key parts of the module\n",
    "    with open(module_path, 'r', encoding='utf-8') as f:\n",
    "        content = f.read()\n",
    "    \n",
    "    # Check for key features\n",
    "    has_bpe = \"BPETokenizer\" in content\n",
    "    has_text_tokens = '<EOS>' in content or '\"<EOS>\"' in content\n",
    "    has_special_handling = \"SPECIAL_TOKENS\" in content\n",
    "    \n",
    "    print(f\"\\nModule Features:\")\n",
    "    print(f\"  BPE Tokenizer class: {'✓' if has_bpe else '✗'}\")\n",
    "    print(f\"  Text-based special tokens: {'✓' if has_text_tokens else '✗'}\")\n",
    "    print(f\"  Special token handling: {'✓' if has_special_handling else '✗'}\")\n",
    "    \n",
    "    # Show character count\n",
    "    print(f\"\\nModule size: {len(content):,} characters\")\n",
    "else:\n",
    "    print(\"WARNING: Module file not found!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
